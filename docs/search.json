[
  {
    "objectID": "week9/index.html",
    "href": "week9/index.html",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#main-reference",
    "href": "week9/index.html#main-reference",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "",
    "text": "brolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R"
  },
  {
    "objectID": "week9/index.html#what-you-will-learn-this-week",
    "href": "week9/index.html#what-you-will-learn-this-week",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week9/index.html#lecture-slides",
    "href": "week9/index.html#lecture-slides",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week9/index.html#tutorial-instructions",
    "href": "week9/index.html#tutorial-instructions",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week9/index.html#assignments",
    "href": "week9/index.html#assignments",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week9/index.html#assignments-1",
    "href": "week9/index.html#assignments-1",
    "title": "Week 9: Exploring data having a space and time context Part I",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September.\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week7/index.html",
    "href": "week7/index.html",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#main-reference",
    "href": "week7/index.html#main-reference",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "",
    "text": "Wilke (2019) Ch 9, 10.2-4, 11.2"
  },
  {
    "objectID": "week7/index.html#what-you-will-learn-this-week",
    "href": "week7/index.html#what-you-will-learn-this-week",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week7/index.html#lecture-slides",
    "href": "week7/index.html#lecture-slides",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week7/index.html#tutorial-instructions",
    "href": "week7/index.html#tutorial-instructions",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week7/index.html#assignments",
    "href": "week7/index.html#assignments",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week7/index.html#assignments-1",
    "href": "week7/index.html#assignments-1",
    "title": "Week 7: Making comparisons between groups and strata",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September."
  },
  {
    "objectID": "week5/index.html",
    "href": "week5/index.html",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#main-reference",
    "href": "week5/index.html#main-reference",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "",
    "text": "Wilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions"
  },
  {
    "objectID": "week5/index.html#what-you-will-learn-this-week",
    "href": "week5/index.html#what-you-will-learn-this-week",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week5/index.html#lecture-slides",
    "href": "week5/index.html#lecture-slides",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week5/index.html#tutorial-instructions",
    "href": "week5/index.html#tutorial-instructions",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week5/index.html#assignments",
    "href": "week5/index.html#assignments",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week5/index.html#assignments-1",
    "href": "week5/index.html#assignments-1",
    "title": "Week 5: Working with a single variable, making transformations, detecting outliers, using robust statistics",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Monday 26 August."
  },
  {
    "objectID": "week3/tutorial.html",
    "href": "week3/tutorial.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorial.html#objectives",
    "href": "week3/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorial.html#preparation",
    "href": "week3/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book ‚ÄúBeautiful Data‚Äù edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorial.html#exercises",
    "href": "week3/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (üõë ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What‚Äôs in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section ‚ÄúThe Rich Get Richer and the Poor Get Poorer‚Äù there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?"
  },
  {
    "objectID": "week3/tutorial.html#finishing-up",
    "href": "week3/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/tutorialsol.html",
    "href": "week3/tutorialsol.html",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorialsol.html#objectives",
    "href": "week3/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 3",
    "section": "",
    "text": "Constructing, planning and evaluating an exploratory data analysis are important skills. This tutorial is an exercise in reading and digesting a really good analysis. Your goal is to understand the analysis, reproduce it, and the choices the analysts made, and why these were would be considered high quality."
  },
  {
    "objectID": "week3/tutorialsol.html#preparation",
    "href": "week3/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 3",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is EDA Case Study: Bay area blues. It is authored by Hadley Wickham, Deborah F. Swayne, and David Poole. It appeared in the book ‚ÄúBeautiful Data‚Äù edited by Jeff Hammerbacher and Toby Segaran. Not all the chapters in the book are good examples of data analysis, though.\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"forcats\", \"patchwork\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week3/tutorialsol.html#exercises",
    "href": "week3/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 3",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nPoint your web browser to the github site for the analysis, https://github.com/hadley/sfhousing. The main data file is house-sales.csv. Read this data into your R session. (üõë ARE YOU USING A PROJECT FOR THIS UNIT? IF NOT, STOP and OPEN IT NOW.)\nYou can read the data in directly from the web site using this code:\n\nlibrary(tidyverse)\nlibrary(patchwork)\nlibrary(forcats)\nsales &lt;- read_csv(\"https://raw.githubusercontent.com/hadley/sfhousing/master/house-sales.csv\")\n\n\n1. What‚Äôs in the data?\n\nIs the data in tidy form?\nOf the variables in the data, which are\n\nnumeric?\ncategorical?\ntemporal?\n\nWhat would be an appropriate plot to make to examine the\n\nnumeric variables?\ncategorical variables?\na categorical and numeric variable?\na temporal variable and a numeric variable?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nYes\n\nprice, br, lsqft, bsqft\ncounty, city, zip, street\nyear, date, datesold\n\n\nscatterplots\nbar charts, pie charts, mosaic\nfacet by the categorical variable. could be boxplots, or density plots, or facetted scatterplots to look at multiple numeric variables\n\ntime series plot, connect lines to indicate time, maybe need to aggregate over time to get one value per time point\n\n\n\n\n\n\n\n\n2. Time series plots\nReproduce the time series plots of weekly average price and volume of sales.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nNote, stacking time series plots helps compare the series relative to the time point.\n\nsales_weekly &lt;- sales |&gt;\n  group_by(date) |&gt;\n  summarise(av_price = mean(price, na.rm=TRUE),\n            volume = n())\np1 &lt;- ggplot(sales_weekly, aes(x=date,\n                               y=av_price)) +\n  geom_line() +\n  scale_y_continuous(\"Average price (millions)\", \n              breaks = seq(500000, 800000, 50000), \n              labels = c(\"0.50\", \"0.55\", \"0.60\", \"0.65\",\n                         \"0.70\", \"0.75\", \"0.80\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np2 &lt;- ggplot(sales_weekly, aes(x=date, y=volume)) + geom_line() +\n  scale_y_continuous(\"Number of sales\", \n              breaks = seq(500,3000,500), \n              labels = c(\"500\", \"1,000\", \"1,500\", \"2,000\",\n                         \"2,500\", \"3,000\")) +\n  scale_x_date(\"\", date_breaks = \"1 years\", \n               minor_breaks = NULL, \n               date_labels = \"%Y\") +\n  theme(aspect.ratio = 0.5)\np1 + p2 + plot_layout(ncol=1)\n\n\n\n\n\n\n\n3. Correlation between series\nIt looks like volume goes down as price goes up. There is a better plot to make to examine this. What is it? Make the plot. After making the plot, report what you learn about the apparent correlation.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales_weekly, aes(x=av_price, y=volume)) +\n  geom_point() +\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nAny correlation is very weak, and negative.\n\n\n\n\n\n\n4. Geographic differences\nThink about potential plots you might make for examining differences by geographic region (as measured by zip, county or city). Make a plot, and report what you learn.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nggplot(sales, \n       aes(x = fct_reorder(county, \n                  price, na.rm=TRUE), \n           y = price)) +\n         geom_boxplot() + \n  scale_y_log10() +\n  xlab(\"\") +\n  coord_flip()\n\n\n\n\n\n\n\n\nMarin County has the highest prices on average, and San Joaquin the lowest. The lowest priced house was sold in Sonoma County. The highest priced properties and lowest priced are pretty similar from one county to another - that is, the variability within county is large.\n\n\n\n\n\n\n5. The Rich Get Richer and the Poor Get Poorer\nIn the section ‚ÄúThe Rich Get Richer and the Poor Get Poorer‚Äù there are some interesting transformations of the data, and unusual types of plots. Explain why looking at proportional change in value refines the view of price movement in different higher vs lower priced properties.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe transformation makes changes relative to the initial average price at the start of the time period. All curves produced will start from the same point. This means that we only need to compare the end points of each line, saving us from calculating differences between lines relative to the difference at the beginning.\n\n\n\n\n\n\n6. Anything surprising?\nWere there any findings that surprised the authors? Or would surprise you?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nI found it interesting that Mountain View had no decline in housing prices. This city has the headquarters of many of the world‚Äôs largest technology companies are in the city, including Google, Mozilla Foundation, Symantec, and Intuit.\n\n\n\n\n\n\n7. Additional resources\nSome of the findings were compared against information gathered from external sources. Can you point to an example of this, and how the other information was used to support or question the finding?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nAll of this is consistent with what we have learned about subprime mortgages since the housing bust hit the headlines.\nSubprime mortgages were offered on little collateral which meant they were quite risky, and they tended to be on the lower end of the housing market. This information was in all the news headlines at the time, and the analysis that these authors have done was checked against the common reporting at the time. The data was consistent with these reports."
  },
  {
    "objectID": "week3/tutorialsol.html#finishing-up",
    "href": "week3/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 3",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/index.html",
    "href": "week3/index.html",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#main-reference",
    "href": "week3/index.html#main-reference",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "",
    "text": "The initial examination of data"
  },
  {
    "objectID": "week3/index.html#what-you-will-learn-this-week",
    "href": "week3/index.html#what-you-will-learn-this-week",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nRole of IDA\nTechniques for\n\ndata screening\ndata cleaning\nimputation\nvalidation\n\nChecking assumptions for hypothesis testing and fitting linear models"
  },
  {
    "objectID": "week3/index.html#lecture-slides",
    "href": "week3/index.html#lecture-slides",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week3/index.html#tutorial-instructions",
    "href": "week3/index.html#tutorial-instructions",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week3/index.html#assignments",
    "href": "week3/index.html#assignments",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week3/index.html#assignments-1",
    "href": "week3/index.html#assignments-1",
    "title": "Week 3: Initial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week2/slides.html#birth-of-eda",
    "href": "week2/slides.html#birth-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Birth of EDA",
    "text": "Birth of EDA\n\nThe field of exploratory data analysis came of age when this book appeared in 1977.\n\nTukey held that too much emphasis in statistics was placed on statistical hypothesis testing (confirmatory data analysis); more emphasis needed to be placed on using data to suggest hypotheses to test."
  },
  {
    "objectID": "week2/slides.html#john-w.-tukey",
    "href": "week2/slides.html#john-w.-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "John W. Tukey",
    "text": "John W. Tukey\n\n\n\n\n\n\n Image source: wikimedia.org\n\n\nBorn in 1915, in New Bedford, Massachusetts.\nMum was a private tutor who home-schooled John. Dad was a Latin teacher.\nBA and MSc in Chemistry, and PhD in Mathematics\nAwarded the National Medal of Science in 1973, by President Nixon\nBy some reports, his home-schooling was unorthodox and contributed to his thinking and working differently."
  },
  {
    "objectID": "week2/slides.html#taking-a-glimpse-back-in-time",
    "href": "week2/slides.html#taking-a-glimpse-back-in-time",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taking a glimpse back in time",
    "text": "Taking a glimpse back in time\nis possible with the American Statistical Association video lending library.\n We‚Äôre going to watch John Tukey talking about exploring high-dimensional data with an amazing new computer in 1973, four years before the EDA book.\n\nLook out for these things:\nTukey‚Äôs expertise is described as for trial and error learning and the computing equipment.\n\n\nFirst 4.25 minutes"
  },
  {
    "objectID": "week2/slides.html#setting-the-frame-of-mind",
    "href": "week2/slides.html#setting-the-frame-of-mind",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Setting the frame of mind",
    "text": "Setting the frame of mind\nExcerpt from the introduction\n\nThis book is based on an important principle.\n It is important to understand what you CAN DO before you learn to measure how WELL you seem to have DONE it.\n Learning first what you can do will help you to work more easily and effectively.\n This book is about exploratory data analysis, about looking at data to see what it seems to say. It concentrates on simple arithmetic and easy-to-draw pictures. It regards whatever appearances we have recognized as partial descriptions, and tries to look beneath them for new insights. Its concern is with appearance, not with confirmation.\n Examples, NOT case histories\n The book does not exist to make the case that exploratory data analysis is useful. Rather it exists to expose its readers and users to a considerable variety of techniques for looking more effectively at one‚Äôs data. The examples are not intended to be complete case histories. Rather they should isolated techniques in action on real data. The emphasis is on general techniques, rather than specific problems. \nA basic problem about any body of data is to make it more easily and effectively handleable by minds ‚Äì our minds, her mind, his mind. To this general end:\n\nanything that make a simpler description possible makes the description more easily handleable.\nanything that looks below the previously described surface makes the description more effective.\n\n\nSo we shall always be glad (a) to simplify description and (b) to describe one layer deeper. In particular,\n\nto be able to say that we looked one layer deeper, and found nothing, is a definite step forward ‚Äì though not as far as to be able to say that we looked deeper and found thus-and-such.\nto be able to say that ‚Äúif we change our point of view in the following way ‚Ä¶ things are simpler‚Äù is always a gain‚Äìthough not quite so much as to be able to say ‚Äúif we don‚Äôt bother to change out point of view (some other) things are equally simple.‚Äù\n\n ‚Ä¶\n Consistent with this view, we believe, is a clear demand that pictures based on exploration of data should force their messages upon us. Pictures that emphasize what we already know‚Äì‚Äúsecurity blankets‚Äù to reassure us‚Äìare frequently not worth the space they take. Pictures that have to be gone over with a reading glass to see the main point are wasteful of time and inadequate of effect. The greatest value of a picture is when it forces us to notice what we never expected to see.\n\n\nConfirmation\n\n\nThe principles and procedures of what we call confirmatory data analysis are both widely used and one of the great intellectual products of our century. In their simplest form, these principles and procedures look at a sample‚Äìand at what that sample has told us about the population from which it came‚Äìand assess the precision with which our inference from sample to population is made. We can no longer get along without confirmatory data analysis. But we need not start with it.\n\nThe best way to understand what CAN be done is not longer‚Äìif it ever was‚Äìto ask what things could, in the current state of our skill techniques, be confirmed (positively or negatively). Even more understanding is lost if we consider each thing we can do to data only in terms of some set of very restrictive assumptions under which that thing is best possible‚Äìassumptions we know we CANNOT check in practice.\n\nExploration AND confirmation\n\nOnce upon a time, statisticians only explored. Then they learned to confirm exactly‚Äìto confirm a few things exactly, each under very specific circumstances. As they emphasized exact confirmation, their techniques inevitably became less flexible. The connection of the most used techniques with past insights was weakened. Anything to which confirmatory procedure was not explicitly attached was decried as ‚Äúmere descriptive statistics‚Äù, no matter how much we learned from it.\n\nToday, the flexibility of (approximate) confirmation by the jacknife makes it relatively easy to ask, for almost any clearly specified exploration, ‚ÄúHow far is it confirmed?‚Äù\n\nToday, exploratory and confirmatory can‚Äìand should‚Äìproceed side by side. This book, of course, considers only exploratory techniques, leaving confirmatory techniques to other accounts.\n\n\n About the problems \n\n The teacher needs to be careful about assigning problems. Not too many, please. They are likely to take longer than you think. The number supplied is to accommodate diversity of interest, not to keep everybody busy.\n Besides the length of our problems, both teacher and student need to realise that many problems do not have a single ‚Äúright answer‚Äù. There can be many ways to approach a body of data. Not all are equally good. For some bodies of data this may be clear, but for others we may not be able to tell from a single body of data which approach is preferred. Even several bodies of data about very similar situations may not be enough to show which approach should be preferred. Accordingly, it will often be quite reasonable for different analysts to reach somewhat different analyses.\n Yet more‚Äìto unlock the analysis of a body of day, to find the good way to approach it, may require a key, whose finding is a creative act. Not everyone can be expected to create the key to any one situation. And to continue to paraphrase Barnum, no one can be expected to create a key to each situation he or she meets.\n To learn about data analysis, it is right that each of us try many things that do not work‚Äìthat we tackle more problems than we make expert analyses of. We often learn less from an expertly done analysis than from one where, by not trying something, we missed‚Äìat least until we were told about it‚Äìan opportunity to learn more. Each teacher needs to recognize this in grading and commenting on problems.\n\n\n Precision\n\nThe teacher who heeds these words and admits that there need be no one correct approach may, I regret to contemplate, still want whatever is done to be digit perfect. (Under such a requirement, the write should still be able to pass the course, but it is not clear whether she would get an ‚ÄúA‚Äù.) One does, from time to time, have to produce digit-perfect, carefully checked results, but forgiving techniques that are not too distributed by unusual data are also, usually, little disturbed by SMALL arithmetic errors. The techniques we discuss here have been chosen to be forgiving. It is hoped, then, that small arithmetic errors will take little off the problem‚Äôs grades, leaving severe penalties for larger errors, either of arithmetic or concept."
  },
  {
    "objectID": "week2/slides.html#outline",
    "href": "week2/slides.html#outline",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Outline",
    "text": "Outline\n\n\n\nScratching down numbers\nSchematic summary\nEasy re-expression\nEffective comparison\nPlots of relationship\nStraightening out plots (using three points)\nSmoothing sequences\nParallel and wandering schematic plots\nDelineations of batches of points\nUsing two-way analyses\n\n\n\n\n\nMaking two-way analyses\nAdvanced fits\nThree way fits\nLooking in two or more ways at batched of points\nCounted fractions\nBetter smoothing\nCounts in bin after bin\nProduct-ratio plots\nShapes of distributions\nMathematical distributions"
  },
  {
    "objectID": "week2/slides.html#looking-at-numbers-with-tukey",
    "href": "week2/slides.html#looking-at-numbers-with-tukey",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Looking at numbers with Tukey",
    "text": "Looking at numbers with Tukey"
  },
  {
    "objectID": "week2/slides.html#scratching-down-numbers",
    "href": "week2/slides.html#scratching-down-numbers",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Scratching down numbers",
    "text": "Scratching down numbers\n\n\nPrices of Chevrolet in the local used car newspaper ads of 1968.\n\noptions(width=20)\nchevrolets &lt;- tibble(\n  prices = c(250, 150, 795, 895, 695, \n               1699, 1499, 1099, 1693, 1166,\n               688, 1333, 895, 1775, 895,\n               1895, 795))\n#chevrolets$prices\n\n\nStem-and-leaf plot: still seen in introductory statistics texts"
  },
  {
    "objectID": "week2/slides.html#section-1",
    "href": "week2/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "First stem-and-leaf, first digit on stem, second digit on leaf\n\n\nOrder any leaves which need it, eg stem 6\n\n\n\n\nA benefit is that the numbers can be read off the plot, but the focus is still on the pattern. Also quantiles like the median, can be computed easily."
  },
  {
    "objectID": "week2/slides.html#section-2",
    "href": "week2/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Shrink the stem\n\n\nShrink the stem more"
  },
  {
    "objectID": "week2/slides.html#and-in-r",
    "href": "week2/slides.html#and-in-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "And, in R ‚Ä¶",
    "text": "And, in R ‚Ä¶\n\nchevrolets$prices\n\n [1]  250  150  795\n [4]  895  695 1699\n [7] 1499 1099 1693\n[10] 1166  688 1333\n[13]  895 1775  895\n[16] 1895  795\n\nstem(chevrolets$prices)\n\n\n  The decimal point is 3 digit(s) to the right of the |\n\n  0 | 23\n  0 | 7788999\n  1 | 123\n  1 | 57789"
  },
  {
    "objectID": "week2/slides.html#remember-the-tips-data",
    "href": "week2/slides.html#remember-the-tips-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üîñ Remember the tips data",
    "text": "üîñ Remember the tips data\n\n\n [1] 1.01 1.66 3.50 3.31 3.61 4.71 2.00 3.12 1.96 3.23 1.71 5.00 1.57 3.00 3.02\n[16] 3.92 1.67 3.71 3.50 3.35 4.08 2.75 2.23 7.58 3.18 2.34 2.00 2.00 4.30 3.00\n[31] 1.45 2.50 3.00 2.45 3.27 3.60 2.00 3.07 2.31 5.00 2.24 2.54 3.06 1.32 5.60\n[46] 3.00 5.00 6.00 2.05 3.00\n\n\n\nstem(tips$tip, scale=0.5, width=120)\n\n\n  The decimal point is at the |\n\n   1 | 000001233334445555555555556666667777788889\n   2 | 000000000000000000000000000000000000000001122222223333555555555555556666677788899\n   3 | 00000000000000000000000011111112222222333344445555555555555666778889\n   4 | 0000000000001112233335777\n   5 | 00000000001122226799\n   6 | 05577\n   7 | 6\n   8 | \n   9 | 0\n  10 | 0"
  },
  {
    "objectID": "week2/slides.html#refining-the-size",
    "href": "week2/slides.html#refining-the-size",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Refining the size",
    "text": "Refining the size\n\n\nFive digits per stem\n\n\n\nWhat is the number in parentheses? And why might this be useful?\n\n\n\nTwo digits per stem\n\n\n\n\n\nstem(tips$tip, scale=2)\n\n\n  The decimal point is 1 digit(s) to the left of the |\n\n   10 | 0000107\n   12 | 55526\n   14 | 44578000000000678\n   16 | 1346781356\n   18 | 032678\n   20 | 00000000000000000000000000000000011233598\n   22 | 0033440114\n   24 | 5700000000002456\n   26 | 01412455\n   28 | 382\n   30 | 00000000000000000000000267891245688\n   32 | 133557159\n   34 | 0188800000000015\n   36 | 0181566\n   38 | 2\n   40 | 0000000000006889\n   42 | 09004\n   44 | 0\n   46 | 713\n   48 | \n   50 | 000000000074567\n   52 | 0\n   54 | \n   56 | 05\n   58 | 52\n   60 | 0\n   62 | \n   64 | 00\n   66 | 03\n   68 | \n   70 | \n   72 | \n   74 | 8\n   76 | \n   78 | \n   80 | \n   82 | \n   84 | \n   86 | \n   88 | \n   90 | 0\n   92 | \n   94 | \n   96 | \n   98 | \n  100 | 0\n\n\n\n\nWhy no number in parentheses?\n\n\n\nmedian(tips$tip)\n\n[1] 2.9"
  },
  {
    "objectID": "week2/slides.html#summary",
    "href": "week2/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\nStem-and-leaf plots are similar information to the histogram.\nGenerally it is possible to also read off the numbers, and to then easily calculate median or Q1 or Q3.\nIt‚Äôs great for small data sets, when you only have pencil and paper.\nAlternatives are a histogram, (jittered) dotplot, density plot, box plot, violin plot, letter value plot."
  },
  {
    "objectID": "week2/slides.html#a-different-style-of-number-scratching",
    "href": "week2/slides.html#a-different-style-of-number-scratching",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "a different style of number scratching",
    "text": "a different style of number scratching\nfor categorical variables\n\n\nWe know about\n\nbut its too easy to\n\nmake a mistake\n\nIs this easier?\n\n\nor harder"
  },
  {
    "objectID": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "href": "week2/slides.html#count-this-data-using-the-squares-approach.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Count this data using the squares approach.",
    "text": "Count this data using the squares approach.\n\n\n\n\n [1] \"F\" \"M\" \"M\" \"M\" \"F\" \"M\"\n [7] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[13] \"M\" \"M\" \"F\" \"M\" \"F\" \"M\"\n[19] \"F\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[25] \"M\" \"M\" \"M\" \"M\" \"M\" \"F\"\n[31] \"M\" \"M\" \"F\" \"F\" \"M\" \"M\"\n[37] \"M\" \"F\" \"M\" \"M\" \"M\" \"M\"\n[43] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[49] \"M\" \"M\" \"M\" \"F\" \"F\" \"M\"\n[55] \"M\" \"M\" \"M\" \"F\" \"M\" \"M\"\n[61] \"M\" \"M\" \"M\" \"M\" \"M\" \"M\"\n[67] \"F\" \"F\" \"M\" \"M\" \"M\" \"F\""
  },
  {
    "objectID": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "href": "week2/slides.html#what-does-it-mean-to-feel-what-the-data-are-like",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What does it mean to ‚Äúfeel what the data are like?‚Äù",
    "text": "What does it mean to ‚Äúfeel what the data are like?‚Äù"
  },
  {
    "objectID": "week2/slides.html#section-3",
    "href": "week2/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "This is a stem and leaf of the height of the highest peak in each of the 50 US states.\n\nThe states roughly fall into three groups.\n\nIt‚Äôs not really surprising, but we can imagine this grouping. Alaska is in a group of its own, with a much higher high peak. Then the Rocky Mountain states, California, Washington and Hawaii also have high peaks, and the rest of the states lump together."
  },
  {
    "objectID": "week2/slides.html#section-4",
    "href": "week2/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Exploratory data analysis is detective work ‚Äì in the purest sense ‚Äì finding and revealing the clues."
  },
  {
    "objectID": "week2/slides.html#section-5",
    "href": "week2/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "More summaries of numerical values"
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summaries",
    "href": "week2/slides.html#hinges-and-5-number-summaries",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summaries",
    "text": "Hinges and 5-number summaries\n\n\n\n\n [1] -3.2 -1.7 -0.4  0.1\n [5]  0.3  1.2  1.5  1.8\n [9]  2.4  3.0  4.3  6.4\n[13]  9.8\n\n\nYou know the median is the middle number. What‚Äôs a hinge?\nThere are 13 data values here, provided already sorted. We are going to write them into a Tukey named down-up-down-up pattern, evenly.\nMedian will be 7th, hinge will be 4th from each end."
  },
  {
    "objectID": "week2/slides.html#hinges-and-5-number-summary",
    "href": "week2/slides.html#hinges-and-5-number-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hinges and 5-number summary",
    "text": "Hinges and 5-number summary\n\n\n\n\n\nHinges are almost always the same as Q1 and Q3"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display",
    "href": "week2/slides.html#box-and-whisker-display",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#box-and-whisker-display-1",
    "href": "week2/slides.html#box-and-whisker-display-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "box-and-whisker display",
    "text": "box-and-whisker display\n\n\nStarting with a 5-number summary"
  },
  {
    "objectID": "week2/slides.html#identified-end-values",
    "href": "week2/slides.html#identified-end-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Identified end values",
    "text": "Identified end values\n\n\n\nWhy are some individual points singled out?\n\n\nRules for this one may be clearer?"
  },
  {
    "objectID": "week2/slides.html#section-6",
    "href": "week2/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Isn‚Äôt this imposing a belief?"
  },
  {
    "objectID": "week2/slides.html#section-7",
    "href": "week2/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "There is no excuse for failing to plot and look\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#fences-and-outside-values",
    "href": "week2/slides.html#fences-and-outside-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fences and outside values",
    "text": "Fences and outside values\n\n\nH-spread: difference between the hinges (we would call this Inter-Quartile Range)\nstep: 1.5 times H-spread\ninner fences: 1 step outside the hinges\nouter fences: 2 steps outside the hinges\nthe value at each end closest to, but still inside the inner fence are ‚Äúadjacent‚Äù\nvalues between an inner fence and its neighbouring outer fence are ‚Äúoutside‚Äù\nvalues beyond outer fences are ‚Äúfar out‚Äù\nthese rules produce a SCHEMATIC PLOT"
  },
  {
    "objectID": "week2/slides.html#new-statistics-trimeans",
    "href": "week2/slides.html#new-statistics-trimeans",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "New statistics: trimeans",
    "text": "New statistics: trimeans\nThe number that comes closest to\n\\[\\frac{\\text{lower hinge} + 2\\times \\text{median} + \\text{upper hinge}}{4}\\] is the trimean.\n \nThink about trimmed means, where we might drop the highest and lowest 5% of observations."
  },
  {
    "objectID": "week2/slides.html#letter-value-plots",
    "href": "week2/slides.html#letter-value-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Letter value plots",
    "text": "Letter value plots\n\n\nWhy break the data into quarters? Why not eighths, sixteenths? k-number summaries?\nWhat does a 7-number summary look like?\n\nHow would you make an 11-number summary?\n\n\nlibrary(lvplot)\np &lt;- ggplot(mpg, \n            aes(class, hwy))\np + geom_lv(aes(fill=..LV..)) + \n  scale_fill_brewer() + \n  coord_flip() + \n  xlab(\"\")"
  },
  {
    "objectID": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "href": "week2/slides.html#box-plots-are-ubiquitous-in-use-today.",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Box plots are ubiquitous in use today.",
    "text": "Box plots are ubiquitous in use today.\n - üêàüê© Mostly used to compare distributions, multiple subsets of the data.\n\nPuts the emphasis on the middle 50% of observations, although variations can put emphasis on other aspects."
  },
  {
    "objectID": "week2/slides.html#easy-re-expression",
    "href": "week2/slides.html#easy-re-expression",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Easy re-expression",
    "text": "Easy re-expression"
  },
  {
    "objectID": "week2/slides.html#logs-square-roots-reciprocals",
    "href": "week2/slides.html#logs-square-roots-reciprocals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Logs, square roots, reciprocals",
    "text": "Logs, square roots, reciprocals\n\n\nWhat you need to know about logs?\n\nhow to find good enough logs fast and easily\nthat equal differences in logs correspond to equal ratios of raw values.\n\n(This means that wherever you find people using products or ratios‚Äì even in such things as price indexes‚Äìusing logs‚Äìthus converting producers to sums and ratios to differences‚Äìis likely to help.)\n\n\nThe most common transformations are logs, sqrt root, reciprocals, reciprocals of square roots\n\n-1, -1/2, +1/2, +1\n\nWhat happened to ZERO?\n\n\nIt turns out that the role of a zero power, is for the purposes of re-expression, neatly solved by the logarithm."
  },
  {
    "objectID": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "href": "week2/slides.html#re-express-to-symmetrize-the-distribution",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Re-express to symmetrize the distribution",
    "text": "Re-express to symmetrize the distribution"
  },
  {
    "objectID": "week2/slides.html#power-ladder",
    "href": "week2/slides.html#power-ladder",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Power ladder",
    "text": "Power ladder\n \n‚¨ÖÔ∏è fix RIGHT-skewed values  \n-2, -1, -1/2, 0 (log), 1/3, 1/2, 1, 2, 3, 4\n\nfix LEFT-skewed values ‚û°Ô∏è"
  },
  {
    "objectID": "week2/slides.html#section-8",
    "href": "week2/slides.html#section-8",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "We now regard re-expression as a tool, something to let us do a better job of grasping. The grasping is done with the eye and the better job is through a more symmetric appearance.\nAnother Tukey wisdom drop"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships",
    "href": "week2/slides.html#linearising-bivariate-relationships",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSurprising observation: The small fluctuations in later years.\nWhat might be possible reasons?"
  },
  {
    "objectID": "week2/slides.html#linearising-bivariate-relationships-1",
    "href": "week2/slides.html#linearising-bivariate-relationships-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linearising bivariate relationships",
    "text": "Linearising bivariate relationships\n  \n\nSee some fluctuations in the early years, too. Note that the log transformation couldn‚Äôt linearise."
  },
  {
    "objectID": "week2/slides.html#section-9",
    "href": "week2/slides.html#section-9",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Whatever the data, we can try to gain by straightening or by flattening.\nWhen we succeed in doing one or both, we almost always see more clearly what is going on."
  },
  {
    "objectID": "week2/slides.html#rules-and-advice",
    "href": "week2/slides.html#rules-and-advice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Rules and advice",
    "text": "Rules and advice\n\n\n\nGraphics are friendly.\nArithmetic often exists to make graphs possible.\nGraphs force us to notice the unexpected; nothing could be more important.\nDifferent graphs show us quite different aspects of the same data.\nThere is no more reason to expect one graph to ‚Äútell all‚Äù than to expect one number to do the same.\n‚ÄúPlotting \\(y\\) against \\(x\\)‚Äù involves significant choices‚Äìhow we express one or both variables can be crucial.\n\n\n\nThe first step in penetrating plotting is to straighten out the dependence or point scatter as much as reasonable.\nPlotting \\(y^2\\), \\(\\sqrt{y}\\), \\(log(y)\\), \\(-1/y\\) or the like instead of \\(y\\) is one plausible step to take in search of straightness.\nPlotting \\(x^2\\), \\(\\sqrt{x}\\), \\(log(x)\\), \\(-1/x\\) or the like instead of \\(x\\) is another.\nOnce the plot is straightened, we can usually gain much by flattening it, usually by plotting residuals.\nWhen plotting scatters, we may need to be careful about how we express \\(x\\) and \\(y\\) in order to avoid concealment by crowding."
  },
  {
    "objectID": "week2/slides.html#section-10",
    "href": "week2/slides.html#section-10",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The book is a digest of üåü tricks and treats üåü of massaging numbers and drafting displays.\nMany of the tools have made it into today‚Äôs analyses in various ways. Many have not.\nNotice the word developments too: froots, fences. Tukey brought you the word ‚Äúsoftware‚Äù\nThe temperament of the book is an inspiration for the mind-set for this unit. There is such delight in working with numbers!\n‚ÄúWe love data!‚Äù"
  },
  {
    "objectID": "week2/slides.html#take-aways",
    "href": "week2/slides.html#take-aways",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Take-aways",
    "text": "Take-aways\n\nTukey‚Äôs approach was a reaction to many years of formalising data analysis using statistical hypothesis testing.\nMethodology development in statistical testing was a reaction to the ad-hoc nature of data analysis.\nComplex machine learning models like neural networks are in reaction to the inability of statistical models to capture highly non-linear relationships, and depend heavily on the data provided.\nExploring data today is in reaction to the need to explain complex models, to support organisations against legal challenges to decisions made from the model\nIt is much easier to accomplish computers.\n‚ÄúExploratory data analysis‚Äù as commonly used today term is unfortunately synonymous with ‚Äúdescriptive statistics‚Äù, but it is truly much more. Understanding its history from Tukey‚Äôs advocation helps you see it is the tooling to discover what you don‚Äôt know."
  },
  {
    "objectID": "week2/slides.html#resources",
    "href": "week2/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nwikipedia\nJohn W. Tukey (1977) Exploratory data analysis\nData coding using tidyverse suite of R packages\nSketching canvases made using fabricerin\n\n\n\n\nETC5521 Lecture 2 | ddde.numbat.space"
  },
  {
    "objectID": "week12/index.html#what-you-will-learn-this-week",
    "href": "week12/index.html#what-you-will-learn-this-week",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week12/index.html#lecture-slides",
    "href": "week12/index.html#lecture-slides",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week12/index.html#tutorial-instructions",
    "href": "week12/index.html#tutorial-instructions",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week12/index.html#assignments",
    "href": "week12/index.html#assignments",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week12/index.html#assignments-1",
    "href": "week12/index.html#assignments-1",
    "title": "Week 12: Extending beyond the data, what can and cannot be inferred more generally, given the data collection",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week10/index.html",
    "href": "week10/index.html",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#main-reference",
    "href": "week10/index.html#main-reference",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "",
    "text": "cubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data"
  },
  {
    "objectID": "week10/index.html#what-you-will-learn-this-week",
    "href": "week10/index.html#what-you-will-learn-this-week",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week10/index.html#lecture-slides",
    "href": "week10/index.html#lecture-slides",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week10/index.html#tutorial-instructions",
    "href": "week10/index.html#tutorial-instructions",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week10/index.html#assignments",
    "href": "week10/index.html#assignments",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week10/index.html#assignments-1",
    "href": "week10/index.html#assignments-1",
    "title": "Week 10: Exploring data having a space and time context Part II",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week1/slides.html#about-this-unit",
    "href": "week1/slides.html#about-this-unit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "About this unit",
    "text": "About this unit"
  },
  {
    "objectID": "week1/slides.html#teaching-team-12",
    "href": "week1/slides.html#teaching-team-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 1/2",
    "text": "Teaching team 1/2\n\n\n\nDi Cook  Distinguished Professor  Monash University \n\nüåê https://dicook.org/\n‚úâÔ∏è ETC5521.Clayton-x@monash.edu\nü¶£ @visnut@aus.social\n\n\nI have a PhD from Rutgers University, NJ, and a Bachelor of Science from University of New England\nI am a Fellow of the American Statistical Association, elected member of the the R Foundation and International Statistical Institute, Past-Editor of the Journal of Computational and Graphical Statistics, and the R Journal.\nMy research is in data visualisation, statistical graphics and computing, with application to sports, ecology and bioinformatics. I likes to develop new methodology and software.\nMy students always work on methods and software that is generally useful for the world. They have been responsible for bringing you the tidyverse suite, knitr, plotly, and many other R packages we regularly use."
  },
  {
    "objectID": "week1/slides.html#teaching-team-22",
    "href": "week1/slides.html#teaching-team-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Teaching team 2/2",
    "text": "Teaching team 2/2\n\n\n\nKrisanat Anukarnsakulchularp  Master of Business Analytics  Monash University \n\nüåê https://github.com/KrisanatA\n‚úâÔ∏è ETC5521.Clayton-x@monash.edu\n\n\nHe has a Bachelor of Actuarial Science, Monash University, 2018 - 2021\nand a Master of Business Analytics, Monash University | 2022 - 2023.\nHe has published the R package animbook\nand hopes to be a PhD student at Monash from 2025.\nThis is his second semester tutoring at Monash, and one of several units working on this semester."
  },
  {
    "objectID": "week1/slides.html#got-a-question-or-a-comment",
    "href": "week1/slides.html#got-a-question-or-a-comment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Got a question, or a comment?",
    "text": "Got a question, or a comment?\n\n‚úã üî° You can ask directly by unmuting yourself, or typing in the chat, of the live lecture.\n\nüíª If watching the recording, please post in the discussion (ED) forum."
  },
  {
    "objectID": "week1/slides.html#welcome",
    "href": "week1/slides.html#welcome",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Welcome!",
    "text": "Welcome!\n\n\nBeyond modelling and prediction, data might have many more stories to tell. Exploring data to uncover patterns and structures, involves both numerical and visual techniques designed to reveal interesting information that may be unexpected. However, an analyst must be cautious not to over-interpret apparent patterns, and to use randomisation tools to assess whether the patterns are real or spurious.\n\n\nlearn to use modern data exploration tools with many different types of contemporary data to uncover interesting structures, unusual relationships and anomalies.\nunderstand how to map out appropriate analyses for a given data set and description, define what we would expect to see in the data, and whether what we see is contrary to expectations.\nbe able to compute null samples in order to test apparent patterns, and to interpret the results using computational methods for statistical inference.\ncritically assess the strength and adequacy of data analysis."
  },
  {
    "objectID": "week1/slides.html#unit-structure",
    "href": "week1/slides.html#unit-structure",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìÖ Unit Structure",
    "text": "üìÖ Unit Structure\n\n\n2 hour lecture üë©‚Äçüè´ Tue 10.00am - noon, on zoom (see moodle for the link) Class is more fun if you can attend live!\n2 x 1.5 hour on-campus tutorial üõ†Ô∏è Wed 9:30-11:00 and Wed 7:30-9:00pm CL_Anc-19.LTB_134 Attendance is expected - this is the chance to practice what is explained in lecture under your tutor‚Äôs guidance."
  },
  {
    "objectID": "week1/slides.html#resources",
    "href": "week1/slides.html#resources",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìö Resources",
    "text": "üìö Resources\nüè° Course homepage: this is where you find the course materials  (lecture slides, tutorials and tutorial solutions) https://ddde.numbat.space/\n\nüà¥ Moodle: this is where you find discussion forum, zoom links, and marks https://learning.monash.edu/course/view.php?id=18864\n\nüß∞ GitHub classroom: this is where you will find assignments, but links to each will be available in moodle. https://classroom.github.com/classrooms/175896553-etc5521-2024-classroom-29a96a"
  },
  {
    "objectID": "week1/slides.html#assessment-part-12",
    "href": "week1/slides.html#assessment-part-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üíØ Assessment Part 1/2",
    "text": "üíØ Assessment Part 1/2\n\nWeekly quizzes (5%) There will be a weekly quiz starting week 2 provided through Moodle. These are a great chance to check your knowledge, and help you prepare for the tutorial and to keep up to date with the weekly course material. Your best 10 scores will be used for your final quiz total. \nAssignment 1 (15%), through GitHub classroom, Due: Aug 5, 11:55pm. This is an individual assessment. \nAssignment 2 (20%), through GitHub classroom, Due: Aug 26, 11:55pm. This is an individual assessment. \nAssignment 3 (20%): through GitHub classroom, Due: Sep 16, 11:55pm. This is an individual assessment. \nAssignment 4, parts 1 and 2 (20% each), through GitHub classroom, Due: Oct 7, 11:55pm and Oct 28, 11:55pm."
  },
  {
    "objectID": "week1/slides.html#github-classroom",
    "href": "week1/slides.html#github-classroom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "GitHub Classroom",
    "text": "GitHub Classroom\nWe are going to use GitHub Classroom (etc5521 2024: Diving Deeper into Data Exploration) to distribute assignment templates and keep track of your assignment progress.\n\nClone the first assignment by clicking on the link given in Moodle.\nOnce you have accepted it, you will get a cloned copy on your own GitHub account. It is a private repo, which means you and the teaching staff will be the only people with access.\nIf you need some help getting started, check this information.\nThe week 1 tutorial is the best place to get help with GitHub."
  },
  {
    "objectID": "week1/slides.html#why-exploratory-data-analysis",
    "href": "week1/slides.html#why-exploratory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Why exploratory data analysis?",
    "text": "Why exploratory data analysis?\n What‚Äôs special about exploring data, in contrast to confirmatory data analysis?\n\nLet‚Äôs look at some common definitions and quotes of ‚Äúexploratory data analysis‚Äù."
  },
  {
    "objectID": "week1/slides.html#section",
    "href": "week1/slides.html#section",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "In statistics, exploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.\n\n\n\nhttps://en.wikipedia.org/wiki/Exploratory_data_analysis"
  },
  {
    "objectID": "week1/slides.html#section-1",
    "href": "week1/slides.html#section-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "EDA is not a formal process with a strict set of rules. More than anything, EDA is a state of mind. During the initial phases of EDA you should feel free to investigate every idea that occurs to you. Some of these ideas will pan out, and some will be dead ends.\n\n\n\nhttps://r4ds.had.co.nz/exploratory-data-analysis.html"
  },
  {
    "objectID": "week1/slides.html#section-2",
    "href": "week1/slides.html#section-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Exploratory Data Analysis (EDA) is an approach/philosophy for data analysis that employs a variety of techniques (mostly graphical) to (1) maximize insight into a data set; (2) uncover underlying structure; (3) extract important variables; (4) detect outliers and anomalies; (5) test underlying assumptions; (6) develop parsimonious models; and (7) determine optimal factor settings.\n\n\n\nhttps://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm"
  },
  {
    "objectID": "week1/slides.html#section-3",
    "href": "week1/slides.html#section-3",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "What is Exploratory Data Analysis (EDA)? (1) How to ensure you are ready to use machine learning algorithms in a project? (2) How to choose the most suitable algorithms for your data set? (3) How to define the feature variables that can potentially be used for machine learning?\n\n\n\nhttps://www.kaggle.com/pavansanagapati/a-simple-tutorial-on-exploratory-data-analysis"
  },
  {
    "objectID": "week1/slides.html#section-4",
    "href": "week1/slides.html#section-4",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "EDA is necessary for the next stage of data research. If there was an analogy to exploratory data analysis, it would be that of a painter examining their tools and available time, before deciding on what best to paint.\n\n\n\nhttps://seleritysas.com/blog/2020/05/08/exploratory-data-analysis-and-its-role-in-improving-business-operations/"
  },
  {
    "objectID": "week1/slides.html#section-5",
    "href": "week1/slides.html#section-5",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "These techniques are typically applied before formal modeling commences and can help inform the development of more complex statistical models. Exploratory techniques are also important for eliminating or sharpening potential hypotheses about the world that can be addressed by the data.\n\n\n\nhttps://www.coursera.org/learn/exploratory-data-analysis#syllabus"
  },
  {
    "objectID": "week1/slides.html#section-6",
    "href": "week1/slides.html#section-6",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The purpose of doing the Exploratory Data Analysis or EDA is to find new information in data. The understanding of EDA that practitioners may not aware of, is the EDA uses a visually-examined dataset to understand and summarize the main characteristics of the dataset without having a prior hypothesis or relying upon statistical models.\n\n\n\nhttps://towardsdatascience.com/if-you-dont-find-anything-new-you-don-t-do-eda-right-d356f9995098"
  },
  {
    "objectID": "week1/slides.html#none-of-these-capture-what-this-course-is-about",
    "href": "week1/slides.html#none-of-these-capture-what-this-course-is-about",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "None of these capture what this course is about",
    "text": "None of these capture what this course is about\n\n\n\n\nhttps://www.gocomics.com/calvinandhobbes/2015/08/26"
  },
  {
    "objectID": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "href": "week1/slides.html#a-simple-example-to-illustrate-exploratory-data-analysis-contrasted-with-a-confirmatory-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "A simple example to illustrate ‚Äúexploratory data analysis‚Äù contrasted with a ‚Äúconfirmatory data analysis‚Äù",
    "text": "A simple example to illustrate ‚Äúexploratory data analysis‚Äù contrasted with a ‚Äúconfirmatory data analysis‚Äù"
  },
  {
    "objectID": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "href": "week1/slides.html#what-are-the-factors-that-affect-tipping-behaviour",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What are the factors that affect tipping behaviour?",
    "text": "What are the factors that affect tipping behaviour?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\nFood servers‚Äô tips in restaurants may be influenced by many factors, including the nature of the restaurant, size of the party, and table locations in the restaurant. Restaurant managers need to know which factors matter when they assign tables to food servers.\n\n\n\nlibrary(tidyverse)\ntips &lt;- read_csv(\"http://ggobi.org/book/data/tips.csv\")"
  },
  {
    "objectID": "week1/slides.html#what-is-tipping",
    "href": "week1/slides.html#what-is-tipping",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What is tipping?",
    "text": "What is tipping?\n\nWhen you‚Äôre dining at a full-service restaurant\n\nTip 20 percent of your full bill.\n\nWhen you grab a cup of coffee\n\nRound up or add a dollar if you‚Äôre a regular or ordered a complicated drink.\n\nWhen you have lunch at a food truck\n\nDrop a few dollars into the tip jar, but a little less than you would at a dine-in spot.\n\nWhen you use a gift card\n\nTip on the total value of the meal, not just what you paid out of pocket.\n\n\n\n\nThe basic rules of tipping that everyone should know about"
  },
  {
    "objectID": "week1/slides.html#recommended-procedure-in-the-book",
    "href": "week1/slides.html#recommended-procedure-in-the-book",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Recommended procedure in the book",
    "text": "Recommended procedure in the book\n\nStep 1: Develop a model\n\nShould the response be tip alone and use the total bill as a predictor?\nShould you create a new variable tip rate and use this as the response?\n\nStep 2: Fit the full model with sex, smoker, day, time and size as predictors\nStep 3: Refine model: Should some variables should be dropped?\nStep 4: Check distribution of residuals\nStep 5: Summarise the model, if X=something, what would be the expected tip"
  },
  {
    "objectID": "week1/slides.html#step-1",
    "href": "week1/slides.html#step-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 1",
    "text": "Step 1\nCalculate tip % as tip/total bill \\(\\times\\) 100\n  \n\ntips &lt;- tips %&gt;%\n  mutate(tip_pct = tip/totbill * 100) \n\n\n\nNote: Creating new variables (sometimes called feature engineering), is a common step in any data analysis."
  },
  {
    "objectID": "week1/slides.html#step-2-fit",
    "href": "week1/slides.html#step-2-fit",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Fit",
    "text": "Step 2 Fit\nFit the full model with all variables\n \n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, sex, smoker, day, time, size) %&gt;%\n  lm(tip_pct ~ ., data=.)"
  },
  {
    "objectID": "week1/slides.html#step-2-model-summary",
    "href": "week1/slides.html#step-2-model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 2 Model summary",
    "text": "Step 2 Model summary\n\n\n\nlibrary(broom)\nlibrary(kableExtra)\ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, \n         p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n20.66\n2.49\n8.29\n0.00\n\n\nsexM\n-0.85\n0.83\n-1.02\n0.31\n\n\nsmokerYes\n0.36\n0.85\n0.43\n0.67\n\n\ndaySat\n-0.18\n1.83\n-0.10\n0.92\n\n\ndaySun\n1.67\n1.90\n0.88\n0.38\n\n\ndayThu\n-1.82\n2.32\n-0.78\n0.43\n\n\ntimeNight\n-2.34\n2.61\n-0.89\n0.37\n\n\nsize\n-0.96\n0.42\n-2.28\n0.02\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.042\n1.5\n0.17\n\n\n\n\n\nü§î Which variable(s) would be considered important for predicting tip %?"
  },
  {
    "objectID": "week1/slides.html#step-3-refine-model",
    "href": "week1/slides.html#step-3-refine-model",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Step 3: Refine model",
    "text": "Step 3: Refine model\n\n\n\ntips_lm &lt;- tips %&gt;%\n  select(tip_pct, size) %&gt;% \n  lm(tip_pct ~ ., data=.) \ntidy(tips_lm) %&gt;% \n  kable(digits=2) %&gt;% \n  kable_styling() \n\n\nglance(tips_lm) %&gt;% \n  select(r.squared, statistic, p.value) %&gt;% \n  kable(digits=3)\n\n\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n18.44\n1.12\n16.5\n0.00\n\n\nsize\n-0.92\n0.41\n-2.2\n0.03\n\n\n\n\n\n\n\n\n\n\n\n\n\nr.squared\nstatistic\np.value\n\n\n\n\n0.02\n5\n0.026"
  },
  {
    "objectID": "week1/slides.html#model-summary",
    "href": "week1/slides.html#model-summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model summary",
    "text": "Model summary\n \n\\[\\widehat{tip %} = 18.44 - 0.92 \\times size\\]\n\n \nAs the size of the dining party increases by one person the tip decreases by approximately 1%."
  },
  {
    "objectID": "week1/slides.html#model-assessment",
    "href": "week1/slides.html#model-assessment",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n   \\(R^2 = 0.02\\).\n\n  This dropped by half from the full model, even though no other variables contributed significantly to the model. It might be a good step to examine interaction terms.\nWhat does \\(R^2 = 0.02\\) mean?"
  },
  {
    "objectID": "week1/slides.html#model-assessment-1",
    "href": "week1/slides.html#model-assessment-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model assessment",
    "text": "Model assessment\n\\(R^2 = 0.02\\) means that size explains just 2% of the variance in tip %. This is a very weak model.\n\nAnd \\(R^2 = 0.04\\) is also a very weak model.\nWhat do the \\(F\\) statistic and \\(p\\)-value mean?\nWhat do the \\(t\\) statistics and \\(p\\)-value associated with model coefficients mean?"
  },
  {
    "objectID": "week1/slides.html#overall-model-significance",
    "href": "week1/slides.html#overall-model-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Overall model significance",
    "text": "Overall model significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\] The \\(F\\) statistic refers to\n\\[ H_o: \\beta_1 = ... = \\beta_7 = 0 ~~ vs ~~ H_a: \\text{at least one is not 0}\\] The \\(p\\)-value is the probability that we observe the given \\(F\\) value or larger, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#term-significance",
    "href": "week1/slides.html#term-significance",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Term significance",
    "text": "Term significance\nAssume that we have a random sample from a population. Assume that the model for the population is\n\\[ \\widehat{tip %} = \\beta_0 + \\beta_1 sexM + ... + \\beta_7 size \\] and we have observed\n\\[ \\widehat{tip %} = b_0 + b_1  sexM + ... + b_7 size \\]\nThe \\(t\\) statistics in the coefficient summary refer to\n\\[ H_o: \\beta_k = 0 ~~ vs ~~ H_a: \\beta_k \\neq 0 \\] The \\(p\\)-value is the probability that we observe the given \\(t\\) value or more extreme, computed assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week1/slides.html#model-diagnostics-md",
    "href": "week1/slides.html#model-diagnostics-md",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Model diagnostics (MD)",
    "text": "Model diagnostics (MD)\nNormally, the final model summary would be accompanied diagnostic plots\n\nobserved vs fitted values to check strength and appropriateness of the fit\nunivariate plot, and normal probability plot, of residuals to check for normality\nin the simple final model like this, the observed vs predictor, with model overlaid would be advised to assess the model relative to the variability around the model\nwhen the final model has more terms, using a partial dependence plot to check the relative relationship between the response and predictors would be recommended."
  },
  {
    "objectID": "week1/slides.html#residual-plots",
    "href": "week1/slides.html#residual-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual plots",
    "text": "Residual plots\n\n\n\ntips_aug &lt;- augment(tips_lm)\nggplot(tips_aug, \n    aes(x=.resid)) + \n  geom_histogram() +\n  xlab(\"residuals\")"
  },
  {
    "objectID": "week1/slides.html#residual-normal-probability-plots",
    "href": "week1/slides.html#residual-normal-probability-plots",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Residual normal probability plots",
    "text": "Residual normal probability plots\n\n\n\nggplot(tips_aug, \n    aes(sample=.resid)) + \n  stat_qq() +\n  stat_qq_line() +\n  xlab(\"residuals\") +\n  theme(aspect.ratio=1)"
  },
  {
    "objectID": "week1/slides.html#fitted-vs-observed",
    "href": "week1/slides.html#fitted-vs-observed",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Fitted vs observed",
    "text": "Fitted vs observed\n\n\n\nggplot(tips_aug, \n    aes(x=.fitted, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  xlab(\"observed\") +\n  ylab(\"fitted\")"
  },
  {
    "objectID": "week1/slides.html#model-in-the-data-space",
    "href": "week1/slides.html#model-in-the-data-space",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "‚ÄúModel-in-the-data-space‚Äù",
    "text": "‚ÄúModel-in-the-data-space‚Äù\n\n\n\nggplot(tips_aug, \n    aes(x=size, y=tip_pct)) + \n  geom_point() +\n  geom_smooth(method=\"lm\", se=FALSE) +\n  ylab(\"tip %\")\n\n\nThe fitted model is overlaid on a plot of the data. This is called ‚Äúmodel-in-the-data-space‚Äù (Wickham et al, 2015).\n\nAll the plots on the previous three slides: histogram of residuals, normal probability plot, fitted vs residuals are considered to be ‚Äúdata-in-the-model-space‚Äù. Stay tuned for more discussion on this later."
  },
  {
    "objectID": "week1/slides.html#section-7",
    "href": "week1/slides.html#section-7",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "The result of this work might leave us with\n\na model that could be used to impose a dining/tipping policy in restaurants (see here)\n\n\nbut it should also leave us with an unease that this policy is based on weak support."
  },
  {
    "objectID": "week1/slides.html#summary",
    "href": "week1/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nPlots as we have just seen, associated with pursuit of an answer to a specific question may be best grouped into the category of ‚Äúmodel diagnostics (MD)‚Äù.\n\nThere are additional categories of plots for data analysis that include initial data analysis (IDA), descriptive statistics. Stay tuned for more on these.\n\n\nA separate and big area for plots of data is for communication, where we already know what is in the data and we want to communicate the information as best possible.\n\nWhen exploring data, we are using data plots to discover things we didn‚Äôt already know."
  },
  {
    "objectID": "week1/slides.html#what-did-this-analysis-miss",
    "href": "week1/slides.html#what-did-this-analysis-miss",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What did this analysis miss?",
    "text": "What did this analysis miss?"
  },
  {
    "objectID": "week1/slides.html#general-strategy-for-exploring-data",
    "href": "week1/slides.html#general-strategy-for-exploring-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "General strategy for EXPLORING DATA",
    "text": "General strategy for EXPLORING DATA\n\n\nIts a good idea to examine the data description, the explanation of the variables, and how the data was collected.\n\nYou need to know what type of variables are in the data in order to decide appropriate choice of plots, and calculations to make.\nData description should have information about data collection methods, so that the extent of what we learn from the data might apply to new data.\n\n\n\nWhat does that look like here?\n\nglimpse(tips)\n\nRows: 244\nColumns: 9\n$ obs     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1‚Ä¶\n$ totbill &lt;dbl&gt; 17.0, 10.3, 21.0, 23.7, 24.6, 25‚Ä¶\n$ tip     &lt;dbl&gt; 1.0, 1.7, 3.5, 3.3, 3.6, 4.7, 2.‚Ä¶\n$ sex     &lt;chr&gt; \"F\", \"M\", \"M\", \"M\", \"F\", \"M\", \"M‚Ä¶\n$ smoker  &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"N‚Ä¶\n$ day     &lt;chr&gt; \"Sun\", \"Sun\", \"Sun\", \"Sun\", \"Sun‚Ä¶\n$ time    &lt;chr&gt; \"Night\", \"Night\", \"Night\", \"Nigh‚Ä¶\n$ size    &lt;dbl&gt; 2, 3, 3, 2, 4, 4, 2, 4, 2, 2, 2,‚Ä¶\n$ tip_pct &lt;dbl&gt; 5.9, 16.1, 16.7, 14.0, 14.7, 18.‚Ä¶\n\n\nLook at the distribution of quantitative variables tips, total bill.\n\n\nExamine the distributions across categorical variables.\nExamine quantitative variables relative to categorical variables"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips",
    "href": "week1/slides.html#distributions-of-tips",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    colour=\"white\")"
  },
  {
    "objectID": "week1/slides.html#because-one-binwidth-is-never-enough",
    "href": "week1/slides.html#because-one-binwidth-is-never-enough",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Because, one binwidth is never enough ‚Ä¶",
    "text": "Because, one binwidth is never enough ‚Ä¶"
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-1",
    "href": "week1/slides.html#distributions-of-tips-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) +\n  geom_histogram(\n    breaks=seq(0.5,10.5,1),  \n    colour=\"white\") + \n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nBig fat bins. Tips are skewed, which means most tips are relatively small."
  },
  {
    "objectID": "week1/slides.html#distributions-of-tips-2",
    "href": "week1/slides.html#distributions-of-tips-2",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Distributions of tips",
    "text": "Distributions of tips\n\n\n\nggplot(tips, \n    aes(x=tip)) + \n  geom_histogram(\n    breaks=seq(0.5,10.5,0.1), \n    colour=\"white\") +\n  scale_x_continuous(\n    breaks=seq(0,11,1))\n\nSkinny bins. Tips are multimodal, and occurring at the full dollar and 50c amounts."
  },
  {
    "objectID": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "href": "week1/slides.html#we-could-also-look-at-total-bill-this-way",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We could also look at total bill this way",
    "text": "We could also look at total bill this way\nbut I‚Äôve already done this, and we don‚Äôt learn anything more about the multiple peaks than waht is learned by plotting tips."
  },
  {
    "objectID": "week1/slides.html#relationship-between-tip-and-total",
    "href": "week1/slides.html#relationship-between-tip-and-total",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relationship between tip and total",
    "text": "Relationship between tip and total\n\n\n\np &lt;- ggplot(tips, \n    aes(x= totbill, y=tip)) + \n  geom_point() + \n  scale_y_continuous(\n    breaks=seq(0,11,1))\np\n\nWhy is total on the x axis? \nShould we add a guideline?"
  },
  {
    "objectID": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "href": "week1/slides.html#add-a-guideline-indicating-common-practice",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Add a guideline indicating common practice",
    "text": "Add a guideline indicating common practice\n\n\n\np &lt;- p + geom_abline(intercept=0, \n              slope=0.2) + \n  annotate(\"text\", x=45, y=10, \n           label=\"20% tip\") \np\n\n\n\n\nMost tips less than 20%: Skin flints vs generous diners\nA couple of big tips\nBanding horizontally is the rounding seen previously"
  },
  {
    "objectID": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "href": "week1/slides.html#we-should-examine-bar-charts-and-mosaic-plots-of-the-categorical-variables-next",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "We should examine bar charts and mosaic plots of the categorical variables next",
    "text": "We should examine bar charts and mosaic plots of the categorical variables next\nbut I‚Äôve already done that, and there‚Äôs not too much of interest there."
  },
  {
    "objectID": "week1/slides.html#relative-to-categorical-variables",
    "href": "week1/slides.html#relative-to-categorical-variables",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Relative to categorical variables",
    "text": "Relative to categorical variables\n\n\n\np + facet_grid(smoker~sex) \n\n\n\n\n\n\n\n\n\n\n\n\nThe bigger bills tend to be paid by men (and females that smoke).\nExcept for three diners, female non-smokers are very consistent tippers, probably around 15-18% though.\nThe variability in the smokers is much higher than for the non-smokers."
  },
  {
    "objectID": "week1/slides.html#isnt-this-interesting",
    "href": "week1/slides.html#isnt-this-interesting",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Isn‚Äôt this interesting?",
    "text": "Isn‚Äôt this interesting?"
  },
  {
    "objectID": "week1/slides.html#procedure-of-eda",
    "href": "week1/slides.html#procedure-of-eda",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Procedure of EDA",
    "text": "Procedure of EDA\n\nWe gained a wealth of insight in a short time.\nUsing nothing but graphical methods we investigated univariate, bivariate, and multivariate relationships.\nWe found both global features and local detail. We saw that\n\ntips were rounded; then we saw the obvious\n\ncorrelation between the tip and the size of the bill, noting the scarcity of generous tippers; finally we\ndiscovered differences in the tipping behavior of male and female smokers and non-smokers.\n\n\nThese are unexpected insights were missed from the analysis that focused solely on the primary question."
  },
  {
    "objectID": "week1/slides.html#what-can-go-wrong",
    "href": "week1/slides.html#what-can-go-wrong",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "What can go wrong?",
    "text": "What can go wrong?"
  },
  {
    "objectID": "week1/slides.html#how-was-data-collected",
    "href": "week1/slides.html#how-was-data-collected",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "How was data collected?",
    "text": "How was data collected?\n\n\nIn one restaurant, a food server recorded the following data on all customers they served during an interval of two and a half months in early 1990.\n\nHow much can you infer about tipping more broadly?\n\n\nTip has a weak but significant relationship with total bill?\nTips have a skewed distribution? (More small tips and fewer large tips?)\nTips tend to be made in nice round numbers.\nPeople generally under-tip?\nSmokers are less reliable tippers."
  },
  {
    "objectID": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "href": "week1/slides.html#ways-to-verify-support-or-refute-generalisations",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Ways to verify, support or refute generalisations",
    "text": "Ways to verify, support or refute generalisations\n\n\n\nexternal information\nother studies/samples\ngood choice of calculations and plots\nall the permutations and subsets of measured variables\ncomputational re-sampling methods (we‚Äôll see these soon)\n\n\n\nPoor data collection methods affects every analysis, including statistical or computational modeling.\n\n\nFor this waiter and the restaurant manager, there is some useful information. Like what?\n\n\nService fee for smokers to ensure consistency?\nAssign waiter to variety of party sizes and composition.\nShifts on different days or time of day (not shown)."
  },
  {
    "objectID": "week1/slides.html#words-of-wisdom",
    "href": "week1/slides.html#words-of-wisdom",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Words of wisdom",
    "text": "Words of wisdom\nFalse discovery is the lesser danger when compared to non-discovery. Non-discovery is the failure to identify meaningful structure, and it may result in false or incomplete modeling. In a healthy scientific enterprise, the fear of non-discovery should be at least as great as the fear of false discovery."
  },
  {
    "objectID": "week1/slides.html#where-do-we-go-from-here",
    "href": "week1/slides.html#where-do-we-go-from-here",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Where do we go from here?",
    "text": "Where do we go from here?\n\nMethods for single, bivariate, multivariate\n\nnumerical variables\ncategorical variables\n\nMethods to accommodate temporal and spatial context\nHow to make effective comparisons\nUtilising computational methods to assess what you see is ‚Äúreal‚Äù"
  },
  {
    "objectID": "week1/slides.html#resources-1",
    "href": "week1/slides.html#resources-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Resources",
    "text": "Resources\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n\nETC5521 Lecture 1 | ddde.numbat.space"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "ETC5521 Resources",
    "section": "",
    "text": "Books and articles\n\nCook and Swayne (2007) Interactive and Dynamic Graphics for Data Analysis, Introduction\nDonoho (2017) 50 Years of Data Science\nStaniak and Biecek (2019) The Landscape of R Packages for Automated Exploratory Data Analysis\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Thu 11:00-12:00, 352 Education Blg, and zoom"
  },
  {
    "objectID": "index.html#lecturerchief-examiner",
    "href": "index.html#lecturerchief-examiner",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "",
    "text": "Professor Di Cook\n\nEmail: etc5521.clayton-x@monash.edu\nConsultation: Thu 11:00-12:00, 352 Education Blg, and zoom"
  },
  {
    "objectID": "index.html#tutors",
    "href": "index.html#tutors",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Tutors",
    "text": "Tutors\n\nKrisanat Anukarnsakulchularp\n\nTutorials: Wed 9:30-11:00am and 7:30-9:00pm CL_Anc-19.LTB_134\nConsultation: Mon 9.30-11:00 am, Menzies W9.20"
  },
  {
    "objectID": "index.html#weekly-schedule",
    "href": "index.html#weekly-schedule",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Weekly schedule",
    "text": "Weekly schedule\n\nLecture: Tues 10-12 on zoom (link in Moodle)\nTutorial: 1.5 hours\nWeekly learning quizzes due each Wednesday 9am, from week 2\n\n\n\n\nWeek\nTopic\nReference\nAssessments\n\n\n\n\n22 Jul\nOverview. Why this course? What is EDA?\nThe Landscape of R Packages for Automated Exploratory Data Analysis\n\n\n\n29 Jul\nLearning from history\nEDA Case Study: Bay area blues\n\n\n\n05 Aug\nInitial data analysis and model diagnostics: Model dependent exploration and how it differs from EDA\nThe initial examination of data\nAssignment 1\n\n\n12 Aug\nUsing computational tools to determine whether what is seen in the data can be assumed to apply more broadly\nWickham et al.¬†(2010) Graphical inference for Infovis\n\n\n\n19 Aug\nWorking with a single variable, making transformations, detecting outliers, using robust statistics\nWilke (2019) Ch 6 Visualizing Amounts; Ch 7 Visualizing distributions\n\n\n\n26 Aug\nBivariate dependencies and relationships, transformations to linearise\nWilke (2019) Ch 12 Visualising associations\nAssignment 2\n\n\n02 Sep\nMaking comparisons between groups and strata\nWilke (2019) Ch 9, 10.2-4, 11.2\n\n\n\n09 Sep\nGoing beyond two variables, exploring high dimensions\nCook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1\n\n\n\n16 Sep\nExploring data having a space and time context Part I\nbrolgar: An R package to BRowse Over Longitudinal Data Graphically and Analytically in R\nAssignment 3\n\n\n23 Sep\nMid-semester break\n\n\n\n\n30 Sep\nExploring data having a space and time context Part II\ncubble: An R Package for Organizing and Wrangling Multivariate Spatio-temporal Data\n\n\n\n07 Oct\nSculpting data using models, checking assumptions, co-dependency and performing diagnostics\nHow to use a tour to check if your model suffers from multicollinearity\nAssignment 4 Part 1\n\n\n14 Oct\nExtending beyond the data, what can and cannot be inferred more generally, given the data collection\n\n\n\n\n28 Oct\n\n\nAssignment 4 Part 2"
  },
  {
    "objectID": "index.html#assessments",
    "href": "index.html#assessments",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Assessments",
    "text": "Assessments\n\nWeekly learning quizzes: 5%\nAssignment 1: Instructions (15%)\nAssignment 2: Instructions (20%)\nAssignment 3: Instructions (20%)\nAssignment 4 part 1: Instructions (20%)\nAssignment 4 part 2: Instructions (20%)"
  },
  {
    "objectID": "index.html#software",
    "href": "index.html#software",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Software",
    "text": "Software\nWe will be using the latest versions of R and RStudio.\nHere is the code to install (most of) the R packages we will be using in this unit.\ninstall.packages(c(\"tidyr\", \"dplyr\", \"readr\", \"readxl\", \"readabs\", \"forcats\", \"tsibble\", \"cubble\", \"lubridate\", \"ggplot2\", \"GGally\", \"ggthemes\", \"sugrrants\", \"ggbeeswarm\", \"plotly\", \"gganimate\", \"tourr\", \"sugarbag\", \"tsibbletalk\", \"visdat\", \"inspectdf\", \"naniar\", \"validate\", \"vcd\", \"mvtnorm\", \"nullabor\", \"visage\", \"forecast\", \"cassowaryr\", \"brolgar\", \"palmerpenguins\", \"housingData\",  \"broom\", \"kableExtra\", \"lvplot\", \"colorspace\", \"patchwork\"), dependencies=TRUE)\nFrom GitHub, install\nremotes::install_github(\"casperhart/detourr\")\nIf you are relatively new to R, working through the materials at https://learnr.numbat.space is an excellent way to up-skill. You are epsecially encouraged to work through Chapter 3, on Troubleshooting and asking for help, because at some point you will need help with your coding, and how you go about this matters and impacts the ability of others to help you.\nThese materials are licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."
  },
  {
    "objectID": "week1/index.html",
    "href": "week1/index.html",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#reading",
    "href": "week1/index.html#reading",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "",
    "text": "The Landscape of R Packages for Automated Exploratory Data Analysis"
  },
  {
    "objectID": "week1/index.html#what-you-will-learn-this-week",
    "href": "week1/index.html#what-you-will-learn-this-week",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nHow exploring data is different from a confirmatory analysis\nGet up and running with GitHub Classroom"
  },
  {
    "objectID": "week1/index.html#lecture-slides",
    "href": "week1/index.html#lecture-slides",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week1/index.html#tutorial-instructions",
    "href": "week1/index.html#tutorial-instructions",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week1/index.html#assignments",
    "href": "week1/index.html#assignments",
    "title": "Week 1: Overview. Why this course? What is EDA?",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week1/tutorialsol.html",
    "href": "week1/tutorialsol.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#objectives",
    "href": "week1/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorialsol.html#preparation",
    "href": "week1/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "üîß Preparation",
    "text": "üîß Preparation\n\nComplete the weekly quiz, before the deadline!\nHave git installed on your laptop so that you can access the test classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, ‚Ä¶ Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorialsol.html#exercises",
    "href": "week1/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week1/tutorialsol.html#ice-breaker",
    "href": "week1/tutorialsol.html#ice-breaker",
    "title": "ETC5521 Tutorial 1",
    "section": "1. Ice breaker",
    "text": "1. Ice breaker\n\nGrab your name tag\n\nFollow the instructions of your tutor to get to know your classmates.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nWhich R package is your favourite? ggplot2 or plotly\nDo you prefer to use geom_boxplot or geom_quasirandom to compare subsets of nuemrical variables?\nHave you used the R package purrr before? Y or N\nWhat does the package profvis do? Visualisation data or Profiling code?"
  },
  {
    "objectID": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "href": "week1/tutorialsol.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "2. How good are your detective skills?",
    "text": "2. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorialsol.html#get-started-using-github-classroom",
    "href": "week1/tutorialsol.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorialsol.html#finishing-up",
    "href": "week1/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week1/tutorial.html",
    "href": "week1/tutorial.html",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#objectives",
    "href": "week1/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 1",
    "section": "",
    "text": "This is the first tutorial meeting of the semester. The goal is to get to know other people in the class with you, and your tutors, and check you‚Äôve got the right skills to get started, and to begin thinking about exploratory data analysis."
  },
  {
    "objectID": "week1/tutorial.html#preparation",
    "href": "week1/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 1",
    "section": "üîß Preparation",
    "text": "üîß Preparation\n\nComplete the weekly quiz, before the deadline!\nHave git installed on your laptop so that you can access the test classroom.\nHave the latest versions of RStudio and R installed on your laptop.\nInstall this list of R packages:\n\n\nCreate an RStudio Project for this unit, called ETC5521. All your work in the tutorials should be conducted in this project. Ideally, your project is organised into folders, one for data, one for tutorial_XX, ‚Ä¶ Each week when you begin your tutorial, open the project."
  },
  {
    "objectID": "week1/tutorial.html#exercises",
    "href": "week1/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 1",
    "section": "üì• Exercises",
    "text": "üì• Exercises"
  },
  {
    "objectID": "week1/tutorial.html#ice-breaker",
    "href": "week1/tutorial.html#ice-breaker",
    "title": "ETC5521 Tutorial 1",
    "section": "1. Ice breaker",
    "text": "1. Ice breaker\n\nGrab your name tag\n\nFollow the instructions of your tutor to get to know your classmates.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nWhich R package is your favourite? ggplot2 or plotly\nDo you prefer to use geom_boxplot or geom_quasirandom to compare subsets of nuemrical variables?\nHave you used the R package purrr before? Y or N\nWhat does the package profvis do? Visualisation data or Profiling code?"
  },
  {
    "objectID": "week1/tutorial.html#how-good-are-your-detective-skills",
    "href": "week1/tutorial.html#how-good-are-your-detective-skills",
    "title": "ETC5521 Tutorial 1",
    "section": "2. How good are your detective skills?",
    "text": "2. How good are your detective skills?\nBeing good at noticing something unexpected or unusual is an important skills for exploratory data analysis. This exercise is designed to practice your detective skills.\nPlay the game alzheimer_test from the fun package by running this code:\nYou will be given 6 tasks to complete. Each one is to find a specific letter hidden among a \\(10\\times 30\\) grid of letters. When you are finished, answer these questions:\n\nWhich task did you THINK was the most difficult?\nWhich task does the DATA say was most difficult based, based on the time taken to answer, tm1.1.j. in your results data?\nSave the dataset to an .rda file.\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n           char1.1.j. char2.1.j.  tm1.1.j.\nans.user.2          M          N 30.839718\nans.user.3          I          T 19.695932\nans.user.5          D          O 17.189302\nans.user.1          O          C 16.534676\nans.user.4          F          E  4.424869\nans.user            9          6  3.812386"
  },
  {
    "objectID": "week1/tutorial.html#get-started-using-github-classroom",
    "href": "week1/tutorial.html#get-started-using-github-classroom",
    "title": "ETC5521 Tutorial 1",
    "section": "3. Get started using GitHub Classroom",
    "text": "3. Get started using GitHub Classroom\n\nIn Moodle go to the Assignment 1 instructions to find the invitation to a GitHub Classroom. Accept this invitation.\nClone the assignment repo to your computer.\nOpen the assign01.html instructions.\nMake a start on loading the data into R."
  },
  {
    "objectID": "week1/tutorial.html#finishing-up",
    "href": "week1/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 1",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week11/index.html",
    "href": "week11/index.html",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#main-reference",
    "href": "week11/index.html#main-reference",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "",
    "text": "How to use a tour to check if your model suffers from multicollinearity"
  },
  {
    "objectID": "week11/index.html#what-you-will-learn-this-week",
    "href": "week11/index.html#what-you-will-learn-this-week",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week11/index.html#lecture-slides",
    "href": "week11/index.html#lecture-slides",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week11/index.html#tutorial-instructions",
    "href": "week11/index.html#tutorial-instructions",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week11/index.html#assignments",
    "href": "week11/index.html#assignments",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week11/index.html#assignments-1",
    "href": "week11/index.html#assignments-1",
    "title": "Week 11: Sculpting data using models, checking assumptions, co-dependency and performing diagnostics",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 4 Part 1 is due on Monday 07 October.\nAssignment 4 Part 2 is due on Monday 28 October."
  },
  {
    "objectID": "week2/index.html",
    "href": "week2/index.html",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#main-reference",
    "href": "week2/index.html#main-reference",
    "title": "Week 2: Learning from history",
    "section": "",
    "text": "EDA Case Study: Bay area blues"
  },
  {
    "objectID": "week2/index.html#what-you-will-learn-this-week",
    "href": "week2/index.html#what-you-will-learn-this-week",
    "title": "Week 2: Learning from history",
    "section": "What you will learn this week",
    "text": "What you will learn this week\n\nThe historical origins of EDA\nPencil and paper methods like stem-and-leaf plots\nHow to symmetrise and linearise your data\nWhere EDA is relevant today"
  },
  {
    "objectID": "week2/index.html#lecture-slides",
    "href": "week2/index.html#lecture-slides",
    "title": "Week 2: Learning from history",
    "section": "Lecture slides",
    "text": "Lecture slides\n\nhtml\npdf\nqmd\nR"
  },
  {
    "objectID": "week2/index.html#tutorial-instructions",
    "href": "week2/index.html#tutorial-instructions",
    "title": "Week 2: Learning from history",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\n\nhtml\nqmd"
  },
  {
    "objectID": "week2/index.html#assignments",
    "href": "week2/index.html#assignments",
    "title": "Week 2: Learning from history",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 1 is due on Monday 05 August."
  },
  {
    "objectID": "week2/tutorialsol.html",
    "href": "week2/tutorialsol.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorialsol.html#objectives",
    "href": "week2/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorialsol.html#preparation",
    "href": "week2/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The Landscape of R Packages for Automated Exploratory Data Analysis. This is a lovely summary of software available that is considered to do exploratory data analysis (EDA). (Note: Dr Cook considers these to be mostly descriptive statistics packages, not exploratory data analysis in the true spirit of the term.) This reading will be the basis of the tutorial exercises today.\n\nComplete the weekly quiz, before the deadline!\nInstall this list of R packages, in addition to what you installed in the previous weeks:\n\n\ninstall.packages(c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\", \"cranlogs\", \"tidyverse\", \"nycflights13\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorialsol.html#exercises",
    "href": "week2/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nThe article lists a number of R packages that might be used for EDA: arsenal, autoEDA, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, summarytools, visdat, xray.\n\n1.\nWhat package had the highest number of CRAN downloads as of 12.07.2019? (Based on the paper.)\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummarytools with 84737\n\n\n\n\n\n\n2.\nOpen up the shiny server for checking download rates at https://hadley.shinyapps.io/cran-downloads/. Which of these packages has the highest download rate over the period Jan 1, 2024-today?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlibrary(cranlogs)\neda_pkgs &lt;- cran_downloads(packages=c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\"), from=\"2024-01-01\", to=lubridate::today())\neda_pkgs |&gt; \n  group_by(package) |&gt;\n  summarise(m=mean(count)) |&gt;\n  arrange(desc(m))\n\n# A tibble: 15 √ó 2\n   package           m\n   &lt;chr&gt;         &lt;dbl&gt;\n 1 summarytools 599.  \n 2 SmartEDA     592.  \n 3 visdat       491.  \n 4 DataExplorer 367.  \n 5 arsenal      222.  \n 6 dlookr        97.5 \n 7 explore       55.1 \n 8 funModeling   45.7 \n 9 inspectdf     43.9 \n10 dataMaid      32.2 \n11 ExPanDaR      16.8 \n12 xray           9.84\n13 exploreR       7.35\n14 RtutoR         2.14\n15 autoEDA        0   \n\n\nsmartEDA was on a roll early in the year, but has virtually disappeared. summarytools is going strong. Interestingly, visdat, high on the list, was developed by Nick Tierney in the years he was at Monash.\n\n\n\n\n\n\n3.\nWhat is an interesting pattern to observe from the time series plot of all the downloads?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe weekly seasonality! There is a regular up/down pattern, that if you zoom in closely - try plotting just a couple of weeks of data - you can see corresponds to week day vs weekend.\n\n\n\n\n\n\n4.\nHow many functions does Staniak and Biecek (2019) say visdat has for doing EDA? Explore what each of them does, by running the example code for each function. What do you think are the features that make visdat a really popular package?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n6; Simple focus, useful functions that apply to a lot of problems.\nIt gives an overview of the variable types, and missing values. This is useful to start a data analysis so that you can start working out what methods might be applied to each, and also whether some variables may have too many missing values to be analysable.\n\nlibrary(visdat)\n# function 1\nvis_dat(airquality)\n\n\n\n\n\n\n\n# function 2\nmessy_vector &lt;- c(TRUE,\n                 \"TRUE\",\n                 \"T\",\n                 \"01/01/01\",\n                 \"01/01/2001\",\n                 NA,\n                 NaN,\n                 \"NA\",\n                 \"Na\",\n                 \"na\",\n                 \"10\",\n                 10,\n                 \"10.1\",\n                 10.1,\n                 \"abc\",\n                 \"$%TG\")\nset.seed(1114)\nmessy_df &lt;- data.frame(var1 = messy_vector,\n                       var2 = sample(messy_vector),\n                       var3 = sample(messy_vector))\nvis_guess(messy_df)\n\n\n\n\n\n\n\n# function 3\nvis_miss(airquality)\n\n\n\n\n\n\n\n# function 4\naq_diff &lt;- airquality\naq_diff[1:10, 1:2] &lt;- NA\nvis_compare(airquality, aq_diff)\n\n\n\n\n\n\n\n# function 5\ndat_test &lt;- tibble::tribble(\n            ~x, ~y,\n            -1,  \"A\",\n            0,  \"B\",\n            1,  \"C\",\n            NA, NA\n            )\n\nvis_expect(dat_test, ~.x == -1)\n\n\n\n\n\n\n\n# function 6\nvis_cor(airquality)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.\nThe package summarytools appears to becoming more favourable relative to visdat. Take a look at this package and explain what tools it has that are not available in visdat.\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nlibrary(summarytools)\ndfSummary(tobacco)\n\nData Frame Summary  \ntobacco  \nDimensions: 1000 x 9  \nDuplicates: 2  \n\n-----------------------------------------------------------------------------------------------------------\nNo   Variable       Stats / Values         Freqs (% of Valid)    Graph                 Valid      Missing  \n---- -------------- ---------------------- --------------------- --------------------- ---------- ---------\n1    gender         1. F                   489 (50.0%)           IIIIIIIIII            978        22       \n     [factor]       2. M                   489 (50.0%)           IIIIIIIIII            (97.8%)    (2.2%)   \n\n2    age            Mean (sd) : 50 (18)    63 distinct values    .     .     . . . :   975        25       \n     [numeric]      min &lt; med &lt; max:                             : : : : : . : : : :   (97.5%)    (2.5%)   \n                    18 &lt; 50 &lt; 80                                 : : : : : : : : : :                       \n                    IQR (CV) : 32 (0.4)                          : : : : : : : : : :                       \n                                                                 : : : : : : : : : :                       \n\n3    age.gr         1. 18-34               258 (26.5%)           IIIII                 975        25       \n     [factor]       2. 35-50               241 (24.7%)           IIII                  (97.5%)    (2.5%)   \n                    3. 51-70               317 (32.5%)           IIIIII                                    \n                    4. 71 +                159 (16.3%)           III                                       \n\n4    BMI            Mean (sd) : 26 (4.5)   974 distinct values             :           974        26       \n     [numeric]      min &lt; med &lt; max:                                     : : :         (97.4%)    (2.6%)   \n                    8.8 &lt; 26 &lt; 39                                        : : :                             \n                    IQR (CV) : 5.7 (0.2)                               : : : : :                           \n                                                                     . : : : : : .                         \n\n5    smoker         1. Yes                 298 (29.8%)           IIIII                 1000       0        \n     [factor]       2. No                  702 (70.2%)           IIIIIIIIIIIIII        (100.0%)   (0.0%)   \n\n6    cigs.per.day   Mean (sd) : 6.8 (12)   37 distinct values    :                     965        35       \n     [numeric]      min &lt; med &lt; max:                             :                     (96.5%)    (3.5%)   \n                    0 &lt; 0 &lt; 40                                   :                                         \n                    IQR (CV) : 11 (1.8)                          :                                         \n                                                                 :   . . . . . .                           \n\n7    diseased       1. Yes                 224 (22.4%)           IIII                  1000       0        \n     [factor]       2. No                  776 (77.6%)           IIIIIIIIIIIIIII       (100.0%)   (0.0%)   \n\n8    disease        1. Hypertension        36 (16.2%)            III                   222        778      \n     [character]    2. Cancer              34 (15.3%)            III                   (22.2%)    (77.8%)  \n                    3. Cholesterol         21 ( 9.5%)            I                                         \n                    4. Heart               20 ( 9.0%)            I                                         \n                    5. Pulmonary           20 ( 9.0%)            I                                         \n                    6. Musculoskeletal     19 ( 8.6%)            I                                         \n                    7. Diabetes            14 ( 6.3%)            I                                         \n                    8. Hearing             14 ( 6.3%)            I                                         \n                    9. Digestive           12 ( 5.4%)            I                                         \n                    10. Hypotension        11 ( 5.0%)                                                      \n                    [ 3 others ]           21 ( 9.5%)            I                                         \n\n9    samp.wgts      Mean (sd) : 1 (0.1)    0.86!: 267 (26.7%)    IIIII                 1000       0        \n     [numeric]      min &lt; med &lt; max:       1.04!: 249 (24.9%)    IIII                  (100.0%)   (0.0%)   \n                    0.9 &lt; 1 &lt; 1.1          1.05!: 324 (32.4%)    IIIIII                                    \n                    IQR (CV) : 0.2 (0.1)   1.06!: 160 (16.0%)    III                                       \n                                           ! rounded                                                       \n-----------------------------------------------------------------------------------------------------------\n\n\nIt has a lot more standard statistical summaries, and outputs the summary in a form that can be embedded nicely in a quarto report.\nIf you go to the package website https://github.com/dcomtois/summarytools it‚Äôs not very promising. There is no package website only the GitHub. There have been no updates for 8 months.\n\n\n\n\n\n\n6.\nWhy do you think the package SmartEDA has gone out of favour?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThe web site looks a little uncared for, missing images on the main GitHub page. It has a nice package pacge, but again pieces are missing.\n\nlibrary(SmartEDA)\nExpData(data=mtcars,type=1)\n\n                                          Descriptions     Value\n1                                   Sample size (nrow)        32\n2                              No. of variables (ncol)        11\n3                    No. of numeric/interger variables        11\n4                              No. of factor variables         0\n5                                No. of text variables         0\n6                             No. of logical variables         0\n7                          No. of identifier variables         0\n8                                No. of date variables         0\n9             No. of zero variance variables (uniform)         0\n10               %. of variables having complete cases 100% (11)\n11   %. of variables having &gt;0% and &lt;50% missing cases    0% (0)\n12 %. of variables having &gt;=50% and &lt;90% missing cases    0% (0)\n13          %. of variables having &gt;=90% missing cases    0% (0)\n\nExpData(data=mtcars,type=2)\n\n   Index Variable_Name Variable_Type Sample_n Missing_Count Per_of_Missing\n1      1           mpg       numeric       32             0              0\n2      2           cyl       numeric       32             0              0\n3      3          disp       numeric       32             0              0\n4      4            hp       numeric       32             0              0\n5      5          drat       numeric       32             0              0\n6      6            wt       numeric       32             0              0\n7      7          qsec       numeric       32             0              0\n8      8            vs       numeric       32             0              0\n9      9            am       numeric       32             0              0\n10    10          gear       numeric       32             0              0\n11    11          carb       numeric       32             0              0\n   No_of_distinct_values\n1                     25\n2                      3\n3                     27\n4                     22\n5                     22\n6                     29\n7                     30\n8                      2\n9                      2\n10                     3\n11                     6\n\n\nIt also has some standard summaries like summarytools which is possibly why it became attractive. And can be used to produce a detailed report on the data, although a report like the one in the next question took ages to complete.\nI‚Äôm not sure why it‚Äôs popularity has dropped. The documentation is not great, there‚Äôs quite a few spelling errors. The functions are (sort of) in CamelCase which is not as attractive as snake_case.\n\n\n\n\n\n\n7.\nThe SmartEDA::ExpReport() and DataExplorer::create_report() are functions that generates a long report when provided with a data set. Try this using the latter function, on the nycflights13 data, using this code:\n\n# DataExplorer\nlibrary(DataExplorer)\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n# Create a big data set\nairlines_all &lt;- flights |&gt; \n  full_join(airlines, by = \"carrier\") |&gt;\n  full_join(planes, by = \"tailnum\", \n            suffix = c(\"_flights\", \"_planes\")) |&gt;\n  full_join(airports, by = c(\"origin\"=\"faa\"), \n            suffix = c(\"_carrier\", \"_origin\")) |&gt;\n  full_join(airports, by = c(\"dest\"=\"faa\"), \n            suffix = c(\"_origin\", \"_dest\"))\n\n\ncreate_report(airlines_all, y = \"arr_delay\")\n\nThe code instructs to use arr_delay as a ‚Äúresponse variable‚Äù. Give some reasons why this report is not very useful!\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nThere is a lot of information, and many of the plots and tables are particularly unreadable.\n\n\n\n\n\nHave your Generative AI assistant suggest what to look at when it is given the variable summary, from:\n\n\nglimpse(airlines_all)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nHere‚Äôs what I got from Claude:\nBased on the dataset you've provided, which appears to be about airline flights, I can suggest several plots and summaries that would be informative. Here are some recommendations:\n\nTime-based analysis:\n\nLine plot of average delays (dep_delay and arr_delay) by month and year\nHeatmap of delays by day of week and hour of day\nBox plots of air_time by month\n\n\nCarrier analysis:\n\nBar plot of flight counts by carrier\nBox plots of delays by carrier\nScatter plot of departure delay vs. arrival delay, colored by carrier\n\n\nRoute analysis:\n\nBar plot of most common routes (origin-dest pairs)\nMap visualization of routes with line thickness indicating frequency\n\n\nDistance analysis:\n\nScatter plot of distance vs. air_time\nHistogram of flight distances\n\n\nAircraft analysis:\n\nBar plot of most common aircraft models\nBox plot of delays by aircraft manufacturer\n\n\nAirport analysis:\n\nBar plot of busiest airports (by departure and arrival)\nMap of airports with point size indicating traffic volume\n\n\nSummaries:\n\nTable of average delays by carrier\nSummary statistics (mean, median, std dev) for air_time and distance\nCorrelation matrix of numeric variables\n\n\nTime series:\n\nTime series decomposition of daily flight counts\n\n\nCategorical data:\n\nStacked bar charts for engine types by manufacturer\n\n\nPerformance metrics:\n\nScatter plot of scheduled arrival time vs. actual arrival time\nIt‚Äôs nicely organise by different types of analysis activities that might be done. Each of these require different ways to aggregate the data, to make summaries or plots by various categorical variables. But it doesn‚Äôt suggest pre-processing that might be necessary, so I asked it to tell me what might be needed:\nBefore creating visualizations and summaries, some pre-processing steps would be beneficial for this dataset. Here are some recommendations:\n\nHandle missing values:\n\nCheck for NA or null values, especially in columns like speed, air_time, and delay fields.\nDecide whether to impute missing values or remove rows with missing data, depending on the extent and nature of missingness.\n\n\nDate and time formatting:\n\nCombine year_flights, month, and day into a single date column.\nEnsure time_hour is properly formatted as a datetime object.\n\n\nCategorical data:\n\nConvert appropriate columns to factors (e.g., carrier, origin, dest, manufacturer).\nCheck for and handle any inconsistent spellings or codings in categorical variables.\n\n\nDelay calculations:\n\nCreate a total_delay column by summing dep_delay and arr_delay.\nConsider creating a binary column for whether a flight was delayed (e.g., over 15 minutes).\n\n\nOutlier detection:\n\nCheck for and possibly handle extreme values in delay times, air_time, and distance.\n\n\nFeature engineering:\n\nCreate a day_of_week column from the date.\nCalculate flight speed from distance and air_time.\nCreate a season column based on month.\n\n\nData type conversions:\n\nEnsure numeric columns are of the appropriate type (int or float).\nConvert time columns (dep_time, arr_time, etc.) to a more usable format if they're not already.\n\n\nGeocoding:\n\nEnsure lat and lon columns for both origin and destination are numeric and contain valid coordinates.\n\n\nConsistency checks:\n\nVerify that arr_time is always later than dep_time.\nCheck that air_time is consistent with the difference between arr_time and dep_time.\n\n\nMerging data:\n\nIf the planes and airports data are in separate datasets, merge them carefully with the main flights data.\n\n\nCreating subsets:\n\nConsider creating smaller datasets for specific analyses (e.g., by year, by carrier) to improve processing speed.\nThis is quite useful!\n\n\n\n\n\n\n8.\nIn a limited fashion, lets work through some areas suggested by Claude.\n\nUse visdat to examine the variable types and missing values. You‚Äôll need to take a sample of the data because there are too many observations to reasonably plot. However, a sample should give reasonable insight on the reliability of most variables. What variables may not be useful because they have too many missing values?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nvis_dat(airlines_all[sample(1:nrow(airlines_all), 1000),])\n\n\n\n\n\n\n\n\nspeed is mostly missing. It definitely cannot be used.\ntype, manufacturer, model, engine, year_planes, engines, seats have a lot of missings, and so maybe unusable. It is possible that these missings are for some types of planes, and other types of planes have more complete records.\n\n\n\n\nFor the rest of these questions, you decide how to process the data, make summaries or plots to provide answers.\n\nWhich carrier had the most flights?\nIs this the same for each month? Or day of the week?\nAre there more departure delays for flights in the morning hours, or evening hours?\nFind an error in the data, e.g.¬†a flight that arrived before it left.\nWith your neighbour in the tutorial come up with one thing that is a bit surprising to you that you can learn from this data. Make sure you state what you expected to see, and why what you saw was then a surprise. (It is possible that you can use the DataExplorer report to look at something you had not thought to examine, as motivation.)\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nairlines_all |&gt; count(carrier, sort=TRUE)\n\n# A tibble: 17 √ó 2\n   carrier     n\n   &lt;chr&gt;   &lt;int&gt;\n 1 UA      58665\n 2 B6      54635\n 3 EV      54173\n 4 DL      48110\n 5 AA      32729\n 6 MQ      26397\n 7 US      20536\n 8 9E      18460\n 9 WN      12275\n10 VX       5162\n11 FL       3260\n12 &lt;NA&gt;     2812\n13 AS        714\n14 F9        685\n15 YV        601\n16 HA        342\n17 OO         32\n\nairlines_all |&gt; \n  filter(carrier %in% c(\"UA\", \"B6\", \"EV\", \"DL\")) |&gt;\n  mutate(carrier = factor(carrier, levels = c(\"UA\", \"B6\", \"EV\", \"DL\"))) |&gt;\n  count(month, carrier) |&gt;\n  mutate(month = factor(month, levels=1:12)) |&gt;\n  pivot_wider(names_from = carrier, \n              values_from = n, \n              values_fill=0)\n\n# A tibble: 12 √ó 5\n   month    UA    B6    EV    DL\n   &lt;fct&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;\n 1 1      4637  4427  4171  3690\n 2 2      4346  4103  3827  3444\n 3 3      4971  4772  4726  4189\n 4 4      5047  4517  4561  4092\n 5 5      4960  4576  4817  4082\n 6 6      4975  4622  4456  4126\n 7 7      5066  4984  4641  4251\n 8 8      5124  4952  4563  4318\n 9 9      4694  4291  4725  3883\n10 10     5060  4361  4908  4093\n11 11     4854  4289  4471  3849\n12 12     4931  4741  4307  4093\n\nairlines_all |&gt; \n  filter(carrier %in% c(\"UA\", \"B6\", \"EV\", \"DL\")) |&gt;\n  mutate(carrier = factor(carrier, levels = c(\"UA\", \"B6\", \"EV\", \"DL\"))) |&gt;\n  count(month, carrier) |&gt;\n  ggplot(aes(x=month, y=n, colour=carrier)) + \n    geom_line() +\n    scale_x_continuous(\"\", breaks = 1:12)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  group_by(hour) |&gt;\n  summarise(dep_delay = mean(dep_delay, na.rm = TRUE)) |&gt;\n  ggplot(aes(x=hour, y=dep_delay)) +\n    geom_point() +\n    geom_smooth(se=F)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  ggplot() +\n    geom_abline(intercept=0, slope=1, colour=\"red\") +\n    geom_point(aes(x=dep_delay, y=arr_delay)) +\n    theme(aspect.ratio=1)\n\n\n\n\n\n\n\nairlines_all |&gt;\n  ggplot() +\n    geom_abline(intercept=0, slope=1, colour=\"red\") +\n    geom_point(aes(x=dep_time, y=arr_time)) +\n    theme(aspect.ratio=1)\n\n\n\n\n\n\n\nairlines_all |&gt; \n  filter(origin == dest)\n\n# A tibble: 0 √ó 42\n# ‚Ñπ 42 variables: year_flights &lt;int&gt;, month &lt;int&gt;, day &lt;int&gt;, dep_time &lt;int&gt;,\n#   sched_dep_time &lt;int&gt;, dep_delay &lt;dbl&gt;, arr_time &lt;int&gt;,\n#   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name_carrier &lt;chr&gt;,\n#   year_planes &lt;int&gt;, type &lt;chr&gt;, manufacturer &lt;chr&gt;, model &lt;chr&gt;,\n#   engines &lt;int&gt;, seats &lt;int&gt;, speed &lt;int&gt;, engine &lt;chr&gt;, name_origin &lt;chr&gt;, ‚Ä¶\n\n\n\nUA, United Airlines\nOoh, September EV had more flights than UA!\nDefinitely evening hours.\nThere are some flights that leave substantially early, maybe not a mistake but worrying for a traveller.\nWhat did you find?\n\n\n\n\n\n\n\n9.\nTable 2 of the Landscape paper summarises the activities of two early phases of the CRISP-DM standard. What does CRISP-DM mean? The implication is that EDA is related to ‚Äúdata understanding‚Äù and ‚Äúdata preparation‚Äù. Would you agree with this or disagree? Why?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nCross-Industry Standard Process for Data Mining; EDA techniques can be useful for some parts of these stages, for example finding outliers, or examining missing value patterns. Some of these steps are important for effective EDA, too, for example, you need to know what types of variables you have in order to decide what types of plots to make.\n\n\n\n\n\n\n10.\nTable 1 of the paper is summarising CRAN downloads and GitHub activity is hard to read. How are the rows sorted? What is the most important information communicated by the table? In what way(s) might revising this table make it easier to read and digest the most important information?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nMost important information is the download rate because the purpose is to know which are the commonly used packages. Sorting rows by downloads makes the table easier to read."
  },
  {
    "objectID": "week2/tutorialsol.html#finishing-up",
    "href": "week2/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week2/tutorial.html",
    "href": "week2/tutorial.html",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorial.html#objectives",
    "href": "week2/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 2",
    "section": "",
    "text": "The purpose of this tutorial is to scope out the software reporting to do EDA in R. We want to understand the capabilities and the limitations."
  },
  {
    "objectID": "week2/tutorial.html#preparation",
    "href": "week2/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 2",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The Landscape of R Packages for Automated Exploratory Data Analysis. This is a lovely summary of software available that is considered to do exploratory data analysis (EDA). (Note: Dr Cook considers these to be mostly descriptive statistics packages, not exploratory data analysis in the true spirit of the term.) This reading will be the basis of the tutorial exercises today.\n\nComplete the weekly quiz, before the deadline!\nInstall this list of R packages, in addition to what you installed in the previous weeks:\n\n\ninstall.packages(c(\"arsenal\", \"autoEDA\", \"DataExplorer\", \"dataMaid\", \"dlookr\", \"ExPanDaR\", \"explore\", \"exploreR\", \"funModeling\", \"inspectdf\", \"RtutoR\", \"SmartEDA\", \"summarytools\", \"visdat\", \"xray\", \"cranlogs\", \"tidyverse\", \"nycflights13\"))\n\n\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week2/tutorial.html#exercises",
    "href": "week2/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 2",
    "section": "üì• Exercises",
    "text": "üì• Exercises\nThe article lists a number of R packages that might be used for EDA: arsenal, autoEDA, DataExplorer, dataMaid, dlookr, ExPanDaR, explore, exploreR, funModeling, inspectdf, RtutoR, SmartEDA, summarytools, visdat, xray.\n\n1.\nWhat package had the highest number of CRAN downloads as of 12.07.2019? (Based on the paper.)\n\n\n2.\nOpen up the shiny server for checking download rates at https://hadley.shinyapps.io/cran-downloads/. Which of these packages has the highest download rate over the period Jan 1, 2024-today?\n\n\n3.\nWhat is an interesting pattern to observe from the time series plot of all the downloads?\n\n\n4.\nHow many functions does Staniak and Biecek (2019) say visdat has for doing EDA? Explore what each of them does, by running the example code for each function. What do you think are the features that make visdat a really popular package?\n\n\n5.\nThe package summarytools appears to becoming more favourable relative to visdat. Take a look at this package and explain what tools it has that are not available in visdat.\n\n\n6.\nWhy do you think the package SmartEDA has gone out of favour?\n\n\n7.\nThe SmartEDA::ExpReport() and DataExplorer::create_report() are functions that generates a long report when provided with a data set. Try this using the latter function, on the nycflights13 data, using this code:\n\n# DataExplorer\nlibrary(DataExplorer)\nlibrary(nycflights13)\nlibrary(tidyverse)\n\n# Create a big data set\nairlines_all &lt;- flights |&gt; \n  full_join(airlines, by = \"carrier\") |&gt;\n  full_join(planes, by = \"tailnum\", \n            suffix = c(\"_flights\", \"_planes\")) |&gt;\n  full_join(airports, by = c(\"origin\"=\"faa\"), \n            suffix = c(\"_carrier\", \"_origin\")) |&gt;\n  full_join(airports, by = c(\"dest\"=\"faa\"), \n            suffix = c(\"_origin\", \"_dest\"))\n\n\ncreate_report(airlines_all, y = \"arr_delay\")\n\nThe code instructs to use arr_delay as a ‚Äúresponse variable‚Äù. Give some reasons why this report is not very useful!\n\nHave your Generative AI assistant suggest what to look at when it is given the variable summary, from:\n\n\nglimpse(airlines_all)\n\n\n\n8.\nIn a limited fashion, lets work through some areas suggested by Claude.\n\nUse visdat to examine the variable types and missing values. You‚Äôll need to take a sample of the data because there are too many observations to reasonably plot. However, a sample should give reasonable insight on the reliability of most variables. What variables may not be useful because they have too many missing values?\n\nFor the rest of these questions, you decide how to process the data, make summaries or plots to provide answers.\n\nWhich carrier had the most flights?\nIs this the same for each month? Or day of the week?\nAre there more departure delays for flights in the morning hours, or evening hours?\nFind an error in the data, e.g.¬†a flight that arrived before it left.\nWith your neighbour in the tutorial come up with one thing that is a bit surprising to you that you can learn from this data. Make sure you state what you expected to see, and why what you saw was then a surprise. (It is possible that you can use the DataExplorer report to look at something you had not thought to examine, as motivation.)\n\n\n\n9.\nTable 2 of the Landscape paper summarises the activities of two early phases of the CRISP-DM standard. What does CRISP-DM mean? The implication is that EDA is related to ‚Äúdata understanding‚Äù and ‚Äúdata preparation‚Äù. Would you agree with this or disagree? Why?\n\n\n10.\nTable 1 of the paper is summarising CRAN downloads and GitHub activity is hard to read. How are the rows sorted? What is the most important information communicated by the table? In what way(s) might revising this table make it easier to read and digest the most important information?"
  },
  {
    "objectID": "week2/tutorial.html#finishing-up",
    "href": "week2/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 2",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week3/slides.html#the-role-of-initial-data-analysis",
    "href": "week3/slides.html#the-role-of-initial-data-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "The role of initial data analysis",
    "text": "The role of initial data analysis\n\n\nThe first thing to do with data is to look at them ‚Ä¶. usually means tabulating and plotting the data in many different ways to see what‚Äôs going on. With the wide availability of computer packages and graphics nowadays there is no excuse for ducking the labour of this preliminary phase, and it may save some red faces later.\nCrowder, M. J. & Hand, D. J. (1990) ‚ÄúAnalysis of Repeated Measures‚Äù"
  },
  {
    "objectID": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "href": "week3/slides.html#initial-data-analysis-and-confirmatory-analysis",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Initial Data Analysis and Confirmatory Analysis",
    "text": "Initial Data Analysis and Confirmatory Analysis\n\n\n\nPrior to conducting a confirmatory data analysis, it is important to conduct an initial data analysis (IDA).\n\n\n\nConfirmatory data analysis (CDA) is focused on statistical inference and includes procedures for:\n\nhypothesis testing,\npredictive modelling,\nparameter estimation including uncertainty,\nmodel selection.\n\n\n\n\n\n\nIDA includes:\n\ndescribing the data and collection procedures\nscrutinise data for errors, outliers, missing observations\ncheck assumptions for confirmatory data analysis\n\n\n\nIDA is sometimes called preliminary data analysis.\n\n\n\n\nIDA is related to exploratory data analysis (EDA) in the sense that it is primarily conducted graphically, and there are few formal tests available."
  },
  {
    "objectID": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "href": "week3/slides.html#taxonomies-are-useful-but-rarely-perfect",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Taxonomies are useful but rarely perfect",
    "text": "Taxonomies are useful but rarely perfect"
  },
  {
    "objectID": "week3/slides.html#objectives-of-ida",
    "href": "week3/slides.html#objectives-of-ida",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Objectives of IDA?",
    "text": "Objectives of IDA?\n\nThe main objective for IDA is to intercept any problems in the data that might adversely affect the confirmatory data analysis.\n\n\n\nThe role of CDA is to answer the intended question(s) that the data were collected for.\n\n\nIDA is often unreported in the data analysis reports or scientific papers, for various reasons. It might not have been done, or it may have been conducted but there was no space in the paper to report on it."
  },
  {
    "objectID": "week3/slides.html#ida-in-government-statistics",
    "href": "week3/slides.html#ida-in-government-statistics",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in government statistics",
    "text": "IDA in government statistics\nThe purpose of data cleaning is to bring data up to a level of quality such that it can reliably be used for the production of statistical models or statements.\nA statistical value chain is constructed by defining a number of meaningful intermediate data products, for which a chosen set of quality attributes are well described.\n\n\n\n\n\nvan der Loo & de Jonge (2018) Statistical Data Cleaning with Applications in R"
  },
  {
    "objectID": "week3/slides.html#ida-in-health-and-medical-data",
    "href": "week3/slides.html#ida-in-health-and-medical-data",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA in health and medical data",
    "text": "IDA in health and medical data\n\nHuebner et al (2018)‚Äôs six steps of IDA: (1) Metadata setup, (2) Data cleaning, (3) Data screening, (4) Initial reporting, (5) Refining and updating the analysis plan, (6) Reporting IDA in documentation."
  },
  {
    "objectID": "week3/slides.html#heed-these-words",
    "href": "week3/slides.html#heed-these-words",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Heed these words",
    "text": "Heed these words\n\n\n\nIDA prepares an analyst for CDA. One needs to be careful about NOT compromising the inference.\n\nHow do you compromise inference?\n\n\nChange your inference or questions based on what you find in IDA.\nOutlier removal or not.\nMissing value imputation choices.\nTreatment of zeros.\nHandling of variable type, categorical temporal.\nLack of multivariate relationship checking, including subsets based on levels of categorical variables.\nChoosing variables and observations.\n\n\n\n\nHow do you avoid these errors?\n\nDocument ALL the IDA, using a reproducible analysis script.\nPre-register your CDA plan, so that your CDA questions do not change.\nDecisions made on outlier removal, variable selection, recoding, sampling, handling of zeros have known affects on results, and are justifiable.\n\nInsure yourself against accusations of data snooping, data dredging, data fishing."
  },
  {
    "objectID": "week3/slides.html#data-screening",
    "href": "week3/slides.html#data-screening",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening"
  },
  {
    "objectID": "week3/slides.html#data-screening-1",
    "href": "week3/slides.html#data-screening-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data screening",
    "text": "Data screening\n\n\n\nIt‚Äôs important to check how the data are understood by the computer.\n\nthat is, checking for data type:\n\nWas the date read in as character?\nWas a factor read in as numeric?\n\n\n\nAlso important for making inference is to know whether the data supports making broader conclusions. How was the data collected? Is it clear what the population of interest is, and that the data is a representative sample of this population?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-12",
    "href": "week3/slides.html#example-checking-the-data-type-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (1/2)",
    "text": "Example: Checking the data type (1/2)\n\n\nlecture3-example.xlsx\n\n\n\n\n\nlibrary(readxl)\nlibrary(here)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"))\ndf\n\n# A tibble: 5 √ó 4\n     id date                loc       temp\n  &lt;dbl&gt; &lt;dttm&gt;              &lt;chr&gt;    &lt;dbl&gt;\n1     1 2010-01-03 00:00:00 New York  42  \n2     2 2010-02-03 00:00:00 New York  41.4\n3     3 2010-03-03 00:00:00 New York  38.5\n4     4 2010-04-03 00:00:00 New York  41.1\n5     5 2010-05-03 00:00:00 New York  39.8\n\n\n\nWhat problems are there with the computer‚Äôs interpretation of data type?\nWhat context specific issues indicate incorrect computer interpretation?"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-22",
    "href": "week3/slides.html#example-checking-the-data-type-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type (2/2)",
    "text": "Example: Checking the data type (2/2)\n\n\n\nlibrary(lubridate)\ndf &lt;- read_excel(here(\"data/lecture3-example.xlsx\"), \n                 col_types = c(\"text\", \n                               \"date\", \n                               \"text\",\n                               \"numeric\"))\n\ndf |&gt; \n  mutate(id = as.factor(id),\n         date = ydm(date)) |&gt;\n  mutate(\n         day = day(date),\n         month = month(date),\n         year = year(date)) \n\n# A tibble: 5 √ó 7\n  id    date       loc       temp   day month  year\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 1     2010-03-01 New York  42       1     3  2010\n2 2     2010-03-02 New York  41.4     2     3  2010\n3 3     2010-03-03 New York  38.5     3     3  2010\n4 4     2010-03-04 New York  41.1     4     3  2010\n5 5     2010-03-05 New York  39.8     5     3  2010\n\n\n\n\nid is now a factor instead of integer\nday, month and year are now extracted from the date\nIs it okay now?\n\n\n\n\nIn the United States, it‚Äôs common to use the date format MM/DD/YYYY (gasps) while the rest of the world commonly uses DD/MM/YYYY or better still YYYY/MM/DD.\n\n\n\n\nIt‚Äôs highly probable that the dates are 1st-5th March and not 3rd of Jan-May.\n\n\n\n\nYou can validate interpretation of temperature using weather database."
  },
  {
    "objectID": "week3/slides.html#example-specifying-the-data-type-with-r",
    "href": "week3/slides.html#example-specifying-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Specifying the data type with R",
    "text": "Example: Specifying the data type with R\n\n\n\nYou can robustify your workflow by ensuring you have a check for the expected data type in your code.\n\n\nxlsx_df &lt;- read_excel(here(\"data/lecture3-example.xlsx\"),\n                 col_types = c(\"text\", \"date\", \"text\", \"numeric\"))  |&gt; \n  mutate(id = as.factor(id), \n         date = as.character(date),\n         date = as.Date(date, format = \"%Y-%d-%m\"))\n\n\n\nread_csv has a broader support for col_types\n\n\ncsv_df &lt;- read_csv(here::here(\"data/lecture3-example.csv\"),\n                 col_types = cols(\n                      id = col_factor(),\n                      date = col_date(format = \"%m/%d/%y\"),\n                      loc = col_character(),\n                      temp = col_double()))\n\n\nThe checks (or coercions) ensure that even if the data are updated, you can have some confidence that any data type error will be picked up before further analysis."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-with-r",
    "href": "week3/slides.html#example-checking-the-data-type-with-r",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type with R",
    "text": "Example: Checking the data type with R\nYou can have a quick glimpse of the data type with:\n\ndplyr::glimpse(xlsx_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0‚Ä¶\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor‚Ä¶\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40\n\ndplyr::glimpse(csv_df)\n\nRows: 5\nColumns: 4\n$ id   &lt;fct&gt; 1, 2, 3, 4, 5\n$ date &lt;date&gt; 2010-03-01, 2010-03-02, 2010-03-03, 2010-03-0‚Ä¶\n$ loc  &lt;chr&gt; \"New York\", \"New York\", \"New York\", \"New Yor‚Ä¶\n$ temp &lt;dbl&gt; 42, 41, 38, 41, 40"
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-type-visually",
    "href": "week3/slides.html#example-checking-the-data-type-visually",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data type visually",
    "text": "Example: Checking the data type visually\n\n\nYou can also visualise the data type with:\n\nlibrary(visdat)\nvis_dat(xlsx_df)\n\n\n\n\n\n\n\n\n\n\nlibrary(inspectdf)\ninspect_types(xlsx_df)  |&gt; \n  show_plot()"
  },
  {
    "objectID": "week3/slides.html#data-cleaning",
    "href": "week3/slides.html#data-cleaning",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning",
    "text": "Data cleaning"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-12",
    "href": "week3/slides.html#data-cleaning-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (1/2)",
    "text": "Data cleaning (1/2)\nData quality checks should be one of the first steps in the data analysis to assess any problems with the data.\nThese include using common or domain knowledge to check if the recorded data have sensible values.\n\n\nAre positive values, e.g.¬†height and weight, recorded as positive values with a plausible range?\nIf the data are counts, do the recorded values contain non-integer values?\nFor compositional data, do the values add up to 100% (or 1)? If not, is that a measurement error or due to rounding? Or is another variable missing?\nDoes the data contain only positives, ie disease occurrences, or warranty claims? If so, what would the no report group look like?"
  },
  {
    "objectID": "week3/slides.html#data-cleaning-22",
    "href": "week3/slides.html#data-cleaning-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data cleaning (2/2)",
    "text": "Data cleaning (2/2)\nIn addition, numerical or graphical summaries may reveal that there is unwanted structure in the data, for example,\n\n\n\nDoes the treatment group have different demographic characteristics to the control group?\nAre the distributions similar between the or training and test sets?\nAre there sufficient measurements for each level of categorical variable, or across the range of numerical variables?\n\n\n\n\nDoes the distribution of the data imply violations of assumptions for the CDA, such as\n\nnon-normality,\ndiscrete rather real-valued, or\ndifferent variance in different domains?\n\n\n\n\n\nData scrutinizing is a process that you get better at with practice and have familiarity with the domain area."
  },
  {
    "objectID": "week3/slides.html#example-checking-the-data-quality",
    "href": "week3/slides.html#example-checking-the-data-quality",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example: Checking the data quality",
    "text": "Example: Checking the data quality\n\n\n\n\n# A tibble: 9 √ó 4\n  id    date       loc        temp\n  &lt;fct&gt; &lt;date&gt;     &lt;chr&gt;     &lt;dbl&gt;\n1 1     2010-03-01 New York   42  \n2 2     2010-03-02 New York   41.4\n3 3     2010-03-03 New York   38.5\n4 4     2010-03-04 New York   41.1\n5 5     2010-03-05 New York   39.8\n6 6     2020-03-01 Melbourne  30.6\n7 7     2020-03-02 Melbourne  17.9\n8 8     2020-03-03 Melbourne  18.6\n9 9     2020-03-04 &lt;NA&gt;       21.3\n\n\n\n\nNumerical or graphical summaries or even just eye-balling the data helps to uncover some data quality issues.\nAny issues here?\n\n\n\n\nThere‚Äôs a missing value in loc.\nTemperature is in Farenheit for New York but Celsius in Melbourne (you can validate this again using external sources)."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-17",
    "href": "week3/slides.html#case-study-world-development-indicators-17",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (1/7)",
    "text": "Case study: World development indicators (1/7)\n\n\n\noptions(width=80)\nraw_dat &lt;- read_csv(here(\"data/world-development-indicators.csv\"), \n                    na = \"..\", n_max = 11935)\nglimpse(raw_dat)\n\nRows: 11,935\nColumns: 54\n$ `Country Name`  &lt;chr&gt; \"Argentina\", \"Argentina\", \"Argentina\", \"Argentina\", \"A‚Ä¶\n$ `Country Code`  &lt;chr&gt; \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\", \"ARG\"‚Ä¶\n$ `Series Name`   &lt;chr&gt; \"Adolescent fertility rate (births per 1,000 women age‚Ä¶\n$ `Series Code`   &lt;chr&gt; \"SP.ADO.TFRT\", \"NV.AGR.TOTL.ZS\", \"ER.H2O.FWTL.ZS\", \"SH‚Ä¶\n$ `1969 [YR1969]` &lt;dbl&gt; 6.4e+01, 9.2e+00, NA, NA, 3.3e+00, NA, 2.2e+01, NA, NA‚Ä¶\n$ `1970 [YR1970]` &lt;dbl&gt; 6.5e+01, 9.6e+00, NA, NA, 3.5e+00, NA, 2.5e+01, NA, NA‚Ä¶\n$ `1971 [YR1971]` &lt;dbl&gt; 6.7e+01, 1.1e+01, NA, NA, 3.7e+00, NA, 2.4e+01, 8.7e+0‚Ä¶\n$ `1972 [YR1972]` &lt;dbl&gt; 6.8e+01, 1.1e+01, NA, NA, 3.6e+00, NA, 1.9e+01, 9.2e+0‚Ä¶\n$ `1973 [YR1973]` &lt;dbl&gt; 7.1e+01, 1.2e+01, NA, NA, 3.7e+00, NA, 2.7e+01, 9.6e+0‚Ä¶\n$ `1974 [YR1974]` &lt;dbl&gt; 7.5e+01, 1.0e+01, NA, NA, 3.7e+00, NA, 3.0e+01, 9.9e+0‚Ä¶\n$ `1975 [YR1975]` &lt;dbl&gt; 7.8e+01, 6.6e+00, NA, NA, 3.6e+00, NA, 2.9e+01, 1.0e+0‚Ä¶\n$ `1976 [YR1976]` &lt;dbl&gt; 8.1e+01, 8.2e+00, NA, NA, 3.8e+00, NA, 2.0e+01, 1.0e+0‚Ä¶\n$ `1977 [YR1977]` &lt;dbl&gt; 8.4e+01, 8.1e+00, 9.5e+00, NA, 3.7e+00, NA, 2.6e+01, 1‚Ä¶\n$ `1978 [YR1978]` &lt;dbl&gt; 8.2e+01, 7.5e+00, NA, NA, 3.8e+00, NA, 2.9e+01, 1.1e+0‚Ä¶\n$ `1979 [YR1979]` &lt;dbl&gt; 8.0e+01, 7.8e+00, NA, NA, 4.0e+00, NA, 3.1e+01, 1.2e+0‚Ä¶\n$ `1980 [YR1980]` &lt;dbl&gt; 7.8e+01, 6.4e+00, NA, NA, 3.9e+00, NA, 3.3e+01, 1.2e+0‚Ä¶\n$ `1981 [YR1981]` &lt;dbl&gt; 7.6e+01, 6.5e+00, NA, NA, 3.6e+00, NA, 4.8e+01, 1.2e+0‚Ä¶\n$ `1982 [YR1982]` &lt;dbl&gt; 7.4e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.2e+0‚Ä¶\n$ `1983 [YR1983]` &lt;dbl&gt; 7.4e+01, 8.7e+00, NA, NA, 3.6e+00, NA, 4.6e+01, 1.3e+0‚Ä¶\n$ `1984 [YR1984]` &lt;dbl&gt; 7.4e+01, 8.3e+00, NA, NA, 3.6e+00, NA, 4.2e+01, 1.3e+0‚Ä¶\n$ `1985 [YR1985]` &lt;dbl&gt; 7.4e+01, 7.6e+00, NA, NA, 3.3e+00, NA, 3.3e+01, 1.3e+0‚Ä¶\n$ `1986 [YR1986]` &lt;dbl&gt; 7.4e+01, 7.8e+00, NA, NA, 3.4e+00, NA, 3.3e+01, 1.3e+0‚Ä¶\n$ `1987 [YR1987]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, NA, 3.7e+00, NA, 4.8e+01, 1.4e+0‚Ä¶\n$ `1988 [YR1988]` &lt;dbl&gt; 7.3e+01, 9.0e+00, NA, NA, 3.8e+00, NA, 4.3e+01, 1.4e+0‚Ä¶\n$ `1989 [YR1989]` &lt;dbl&gt; 7.3e+01, 9.6e+00, NA, NA, 3.6e+00, NA, 8.0e+01, 1.3e+0‚Ä¶\n$ `1990 [YR1990]` &lt;dbl&gt; 7.3e+01, 8.1e+00, NA, 9.7e+01, 3.4e+00, NA, 3.2e+01, 1‚Ä¶\n$ `1991 [YR1991]` &lt;dbl&gt; 7.3e+01, 6.7e+00, NA, NA, 3.5e+00, NA, 2.3e+01, 1.3e+0‚Ä¶\n$ `1992 [YR1992]` &lt;dbl&gt; 7.3e+01, 6.0e+00, NA, 9.6e+01, 3.6e+00, NA, 2.2e+01, 1‚Ä¶\n$ `1993 [YR1993]` &lt;dbl&gt; 7.3e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.6e+01, 1.5e+0‚Ä¶\n$ `1994 [YR1994]` &lt;dbl&gt; 7.2e+01, 5.1e+00, NA, NA, 3.5e+00, NA, 2.7e+01, 1.6e+0‚Ä¶\n$ `1995 [YR1995]` &lt;dbl&gt; 7.1e+01, 5.4e+00, NA, 9.8e+01, 3.7e+00, NA, 2.8e+01, 1‚Ä¶\n$ `1996 [YR1996]` &lt;dbl&gt; 7.0e+01, 5.6e+00, NA, NA, 3.8e+00, NA, 2.8e+01, 1.7e+0‚Ä¶\n$ `1997 [YR1997]` &lt;dbl&gt; 7.0e+01, 5.2e+00, 9.8e+00, 9.7e+01, 3.9e+00, NA, 3.0e+‚Ä¶\n$ `1998 [YR1998]` &lt;dbl&gt; 6.9e+01, 5.3e+00, NA, 9.8e+01, 3.9e+00, NA, 3.3e+01, 2‚Ä¶\n$ `1999 [YR1999]` &lt;dbl&gt; 6.8e+01, 4.5e+00, NA, 9.8e+01, 4.0e+00, NA, 3.6e+01, 2‚Ä¶\n$ `2000 [YR2000]` &lt;dbl&gt; 6.7e+01, 4.7e+00, NA, 9.9e+01, 3.8e+00, NA, 3.4e+01, 2‚Ä¶\n$ `2001 [YR2001]` &lt;dbl&gt; 6.6e+01, 4.6e+00, NA, 9.8e+01, 3.6e+00, 6.5e+01, 3.7e+‚Ä¶\n$ `2002 [YR2002]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.3e+00, NA, 6.2e+01, 2‚Ä¶\n$ `2003 [YR2003]` &lt;dbl&gt; 6.5e+01, 1.0e+01, NA, 9.9e+01, 3.5e+00, NA, 5.1e+01, 2‚Ä¶\n$ `2004 [YR2004]` &lt;dbl&gt; 6.4e+01, 8.4e+00, NA, 9.9e+01, 4.1e+00, NA, 4.2e+01, 2‚Ä¶\n$ `2005 [YR2005]` &lt;dbl&gt; 6.4e+01, 7.9e+00, NA, 9.9e+01, 4.1e+00, 7.9e+01, 3.5e+‚Ä¶\n$ `2006 [YR2006]` &lt;dbl&gt; 6.4e+01, 6.9e+00, NA, 9.9e+01, 4.4e+00, NA, 2.8e+01, 2‚Ä¶\n$ `2007 [YR2007]` &lt;dbl&gt; 6.4e+01, 7.5e+00, NA, 9.9e+01, 4.4e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2008 [YR2008]` &lt;dbl&gt; 6.4e+01, 7.3e+00, NA, 9.5e+01, 4.7e+00, NA, 2.2e+01, 2‚Ä¶\n$ `2009 [YR2009]` &lt;dbl&gt; 6.4e+01, 5.3e+00, NA, 9.8e+01, 4.4e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2010 [YR2010]` &lt;dbl&gt; 6.4e+01, 7.1e+00, NA, 9.5e+01, 4.6e+00, NA, 2.5e+01, 2‚Ä¶\n$ `2011 [YR2011]` &lt;dbl&gt; 6.4e+01, 7.0e+00, NA, 9.7e+01, 4.6e+00, NA, 2.6e+01, 2‚Ä¶\n$ `2012 [YR2012]` &lt;dbl&gt; 6.4e+01, 5.8e+00, 1.3e+01, 9.8e+01, 4.6e+00, 5.5e+01, ‚Ä¶\n$ `2013 [YR2013]` &lt;dbl&gt; 6.4e+01, 6.1e+00, NA, 9.7e+01, 4.5e+00, 8.1e+01, 3.3e+‚Ä¶\n$ `2014 [YR2014]` &lt;dbl&gt; 6.4e+01, 6.7e+00, 1.3e+01, 1.0e+02, 4.7e+00, NA, 3.4e+‚Ä¶\n$ `2015 [YR2015]` &lt;dbl&gt; 6.3e+01, 5.2e+00, NA, 1.0e+02, NA, NA, 4.0e+01, NA, NA‚Ä¶\n$ `2016 [YR2016]` &lt;dbl&gt; 6.3e+01, 6.4e+00, NA, NA, NA, NA, 3.8e+01, NA, NA, 1.3‚Ä¶\n$ `2017 [YR2017]` &lt;dbl&gt; NA, 5.6e+00, NA, NA, NA, NA, 3.9e+01, NA, NA, 1.1e+01,‚Ä¶\n$ `2018 [YR2018]` &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA‚Ä¶\n\n\nWorld Development Indicators (WDI), sourced from the World Bank Group (2019)\n\n\n\nWhat are the data types?\nWhat are the variables?\nWhat are the observations?\nIs the data in tidy form?"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-27",
    "href": "week3/slides.html#case-study-world-development-indicators-27",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (2/7)",
    "text": "Case study: World development indicators (2/7)\n\n\n\ncountry_code_df &lt;- raw_dat  |&gt;\n  distinct(`Country Name`, `Country Code`)  |&gt;\n  rename_all(janitor::make_clean_names)  |&gt;\n  left_join(\n    countrycode::codelist |&gt; select(iso3c, region, continent),\n    by = c(\"country_code\" = \"iso3c\")\n  )  |&gt;\n  arrange(continent, region) \n\n\n\n\nRows: 217\nColumns: 4\n$ country_name &lt;chr&gt; \"Algeria\", \"Djibouti\", \"Egypt, Arab Rep.\", \"Libya\", \"Moro‚Ä¶\n$ country_code &lt;chr&gt; \"DZA\", \"DJI\", \"EGY\", \"LBY\", \"MAR\", \"TUN\", \"AGO\", \"BEN\", \"‚Ä¶\n$ region       &lt;chr&gt; \"Middle East & North Africa\", \"Middle East & North Africa‚Ä¶\n$ continent    &lt;chr&gt; \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa\", \"Africa‚Ä¶\n\n\n# A tibble: 6 √ó 2\n  continent     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Africa       54\n2 Americas     46\n3 Asia         50\n4 Europe       46\n5 Oceania      19\n6 &lt;NA&gt;          2\n\n\n# A tibble: 8 √ó 2\n  region                         n\n  &lt;chr&gt;                      &lt;int&gt;\n1 East Asia & Pacific           37\n2 Europe & Central Asia         56\n3 Latin America & Caribbean     42\n4 Middle East & North Africa    21\n5 North America                  3\n6 South Asia                     8\n7 Sub-Saharan Africa            48\n8 &lt;NA&gt;                           2\n\n\n\n\n\n\nHow many countries are included\nHow many continents, regions?\nWhy are there NAs here?\n\n\n\ncountry_code_df |&gt; filter(is.na(continent))\n\n# A tibble: 2 √ó 4\n  country_name    country_code region continent\n  &lt;chr&gt;           &lt;chr&gt;        &lt;chr&gt;  &lt;chr&gt;    \n1 Channel Islands CHI          &lt;NA&gt;   &lt;NA&gt;     \n2 Kosovo          XKX          &lt;NA&gt;   &lt;NA&gt;"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-37",
    "href": "week3/slides.html#case-study-world-development-indicators-37",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (3/7)",
    "text": "Case study: World development indicators (3/7)\n\n\n\nwdi_vars &lt;- raw_dat  |&gt;\n  select(`Series Name`, `Series Code`) |&gt;\n  distinct() |&gt;\n  rename_all(janitor::make_clean_names) \n\n\n\n\n\n\n\n\n\n\n\n\nAnalysis will use the short name (series_code) for variables.\nStore full variable name (series_name) and short name (series_code) in a separate table.\nThe series_code will be used as the key whenever the full name is needed."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-47",
    "href": "week3/slides.html#case-study-world-development-indicators-47",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (4/7)",
    "text": "Case study: World development indicators (4/7)\n\n\n\nwdi &lt;- raw_dat  |&gt;\n  select(`Country Code`, `Series Code`, `1969 [YR1969]`:`2018 [YR2018]`) |&gt;\n  rename_all(janitor::make_clean_names) |&gt;\n  pivot_longer(x1969_yr1969:x2018_yr2018,\n               names_to = \"year\", \n               values_to = \"value\") |&gt;\n  mutate(year = as.numeric(str_sub(year, 2, 5)) ) |&gt;\n  pivot_wider(names_from = series_code,\n              values_from = value)\n\nwdi2017 &lt;- wdi  |&gt; filter(year == 2017)\n\n\nOrganise data into tidy form\nCheck missing value distribution"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-57",
    "href": "week3/slides.html#case-study-world-development-indicators-57",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (5/7)",
    "text": "Case study: World development indicators (5/7)\n\n\nCheck missings by\n\nvariable\ncountry"
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67",
    "href": "week3/slides.html#case-study-world-development-indicators-67",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nLook at Costa Rica (CRI), most complete country\n\n\n\n\n\n\n\n\n\n\nTo illustrate imputation, we‚Äôll show one of the variables, that is relatively complete.\n\n\n\n\n\n\n\n\n\nImpute a few temporal missings using nearest neighbours."
  },
  {
    "objectID": "week3/slides.html#case-study-world-development-indicators-67-1",
    "href": "week3/slides.html#case-study-world-development-indicators-67-1",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: World development indicators (6/7)",
    "text": "Case study: World development indicators (6/7)\n\n\nMissings imputed using imputeTS using the moving average method.\n\n\n\n\n\n\n\n\n\n\n\nDon‚Äôt have to impute before scrutinizing data\nWhat are these numbers supposed to be?\n\nSE.PRM.CMPT.ZS is ‚ÄúPrimary completion rate, total (% of relevant age group)‚Äù\nDo we have any problems?\n\nYes. The explanation of the variable suggests the numbers should range between 0-100."
  },
  {
    "objectID": "week3/slides.html#summary-of-the-process",
    "href": "week3/slides.html#summary-of-the-process",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìã Summary of the process",
    "text": "üìã Summary of the process\nThe steps we took roughly followed these:\n\n\n\n\n\n\n\n\nAt the end of this stage we would have:\n\n3 tables of data: country name/code, variables name/key, time series of multiple variables for many countries\nWhat would you like to learn from this data? What sort of models might be fitted? What types of hypotheses might be tested?\nHave we done anything that might have compromised the later analysis?"
  },
  {
    "objectID": "week3/slides.html#data-collection",
    "href": "week3/slides.html#data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Data collection",
    "text": "Data collection"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-13",
    "href": "week3/slides.html#case-study-employment-data-in-australia-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (1/3)",
    "text": "Case study: Employment Data in Australia (1/3)\nBelow is the data from ABS that shows the total number of people employed in a given month from February 1976 to December 2019 using the original time series.\n\n\nload(here(\"data/employed.rda\"))\nglimpse(employed)\n\nRows: 557\nColumns: 4\n$ date  &lt;date&gt; 1978-02-01, 1978-03-01, 1978-04-01, 1978-05-01, 1978-06-01, 197‚Ä¶\n$ month &lt;dbl&gt; 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1, 2, 3, 4, 5, 6, 7, 8, 9, 1‚Ä¶\n$ year  &lt;dbl&gt; 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978, 1978‚Ä¶\n$ value &lt;dbl&gt; 5986, 6041, 6054, 6038, 6031, 6036, 6005, 6024, 6046, 6034, 6125‚Ä¶\n\n\nAustralian Bureau of Statistics, Labour force, Australia, Table 01. Labour force status by Sex, Australia - Trend, Seasonally adjusted and Original"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-23",
    "href": "week3/slides.html#case-study-employment-data-in-australia-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (2/3)",
    "text": "Case study: Employment Data in Australia (2/3)\n\n\nDo you notice anything?\n\n\n\n\n\n\n\n\n\n\n\nWhy do you think the number of people employed is going up each year?\n\n\n\nAustralian population is 25.39 million in 2019\n1.5% annual increase in population\nVic population is 6.681 million (Sep 2020) - 26%\nNSW population is 8.166 (Sep 2020) - 32%"
  },
  {
    "objectID": "week3/slides.html#case-study-employment-data-in-australia-33",
    "href": "week3/slides.html#case-study-employment-data-in-australia-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Employment Data in Australia (3/3)",
    "text": "Case study: Employment Data in Australia (3/3)\n\n\n\nThere‚Äôs a suspicious change in August numbers from 2014.\n\n\n\n\n\n\n\n\n\n\n\n\n\nA potential explanation for this is that there was a change in the survey from 2014.\n\nSee discussion on this at Hyndsight blog (10 October 2014)."
  },
  {
    "objectID": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "href": "week3/slides.html#case-study-2014-data-mining-cup-winners",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: 2014 Data Mining Cup winners",
    "text": "Case study: 2014 Data Mining Cup winners\n\n\n\nUgly plot of all observations provided in training sample, with response variable in colour, and test sample to predict.\nWhat does this tell you about the test sample?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-12",
    "href": "week3/slides.html#case-study-french-frieshot-chips-12",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (1/2)",
    "text": "Case study: french fries/hot chips (1/2)\n\n\n\n\nRows: 696\nColumns: 9\n$ time      &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ treatment &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ‚Ä¶\n$ subject   &lt;fct&gt; 3, 3, 10, 10, 15, 15, 16, 16, ‚Ä¶\n$ rep       &lt;dbl&gt; 1, 2, 1, 2, 1, 2, 1, 2, 1, 2, ‚Ä¶\n$ potato    &lt;dbl&gt; 2.9, 14.0, 11.0, 9.9, 1.2, 8.8‚Ä¶\n$ buttery   &lt;dbl&gt; 0.0, 0.0, 6.4, 5.9, 0.1, 3.0, ‚Ä¶\n$ grassy    &lt;dbl&gt; 0.0, 0.0, 0.0, 2.9, 0.0, 3.6, ‚Ä¶\n$ rancid    &lt;dbl&gt; 0.0, 1.1, 0.0, 2.2, 1.1, 1.5, ‚Ä¶\n$ painty    &lt;dbl&gt; 5.5, 0.0, 0.0, 0.0, 5.1, 2.3, ‚Ä¶\n\n\n\n10 week sensory experiment, 12 individuals assessed taste of french fries on several scales (how potato-y, buttery, grassy, rancid, paint-y do they taste?), fried in one of 3 different oils, replicated twice.\n\nIs the design complete?\nAre replicates like each other?\nHow do the ratings on the different scales differ?\nAre raters giving different scores on average?\nDo ratings change over the weeks?"
  },
  {
    "objectID": "week3/slides.html#case-study-french-frieshot-chips-22",
    "href": "week3/slides.html#case-study-french-frieshot-chips-22",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: french fries/hot chips (2/2)",
    "text": "Case study: french fries/hot chips (2/2)\n\n\n\nIs the design complete?\n\n\nfrench_fries |&gt; count(subject)\n\n# A tibble: 12 √ó 2\n   subject     n\n   &lt;fct&gt;   &lt;int&gt;\n 1 3          54\n 2 10         60\n 3 15         60\n 4 16         60\n 5 19         60\n 6 31         54\n 7 51         60\n 8 52         60\n 9 63         60\n10 78         60\n11 79         54\n12 86         54\n\n\n\n\n\nfrench_fries |&gt; count(time)\n\n# A tibble: 10 √ó 2\n   time      n\n   &lt;fct&gt; &lt;int&gt;\n 1 1        72\n 2 2        72\n 3 3        72\n 4 4        72\n 5 5        72\n 6 6        72\n 7 7        72\n 8 8        72\n 9 9        60\n10 10       60\n\nfrench_fries |&gt; count(treatment)\n\n# A tibble: 3 √ó 2\n  treatment     n\n  &lt;fct&gt;     &lt;int&gt;\n1 1           232\n2 2           232\n3 3           232\n\nfrench_fries |&gt; count(rep)\n\n# A tibble: 2 √ó 2\n    rep     n\n  &lt;dbl&gt; &lt;int&gt;\n1     1   348\n2     2   348"
  },
  {
    "objectID": "week3/slides.html#case-study-warranty-claims",
    "href": "week3/slides.html#case-study-warranty-claims",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Warranty claims",
    "text": "Case study: Warranty claims\n\n\n\n\nRows: 4,561\nColumns: 14\n$ Region           &lt;chr&gt; \"East\", \"West\", \"North ‚Ä¶\n$ State            &lt;chr&gt; \"Delhi\", \"Gujarat\", \"We‚Ä¶\n$ Area             &lt;chr&gt; \"Urban\", \"Rural\", \"Urba‚Ä¶\n$ City             &lt;chr&gt; \"New Delhi\", \"Ahmedabad‚Ä¶\n$ Consumer_profile &lt;chr&gt; \"Personal\", \"Personal\",‚Ä¶\n$ TV_2001_Issue    &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 1, 1,‚Ä¶\n$ TV_2002_Issue    &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1,‚Ä¶\n$ TV_2003_Issue    &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 0,‚Ä¶\n$ Claim_Value      &lt;dbl&gt; 25000, 4216, 4000, 5000‚Ä¶\n$ Service_Centre   &lt;dbl&gt; 13, 10, 10, 12, 10, 10,‚Ä¶\n$ Product_Age      &lt;dbl&gt; 60, 672, 275, 10, 4, 34‚Ä¶\n$ Purchased_from   &lt;chr&gt; \"Dealer\", \"Dealer\", \"De‚Ä¶\n$ Call_details     &lt;dbl&gt; 1.3, 25.0, 11.0, 1.6, 0‚Ä¶\n$ Purpose          &lt;chr&gt; \"Complaint\", \"Other\", \"‚Ä¶\n\n\n\nTV_2001_Issue: failure of power supply\nTV_2002_Issue: failure of inverter\nTV_2003_Issue: failure of motherboard\n\n\n\n\nWhat is the population that this data is measuring?\nWhat is not measured?\n\n\n\n\n# A tibble: 2 √ó 2\n  City          n\n  &lt;chr&gt;     &lt;int&gt;\n1 Delhi       106\n2 Bangalore   320\n\n\nCan we say that Delhi has fewer problems with TVs than Bangalore?\n\n\n\nSource: ExcelR Projects. (2019). Warranty Claims. Kaggle."
  },
  {
    "objectID": "week3/slides.html#summary-of-checks-for-data-collection",
    "href": "week3/slides.html#summary-of-checks-for-data-collection",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "üìã Summary of checks for data collection",
    "text": "üìã Summary of checks for data collection\n‚úÖ Has the collection process been consistent?\n‚úÖ Does the set to be predicted match the training set?\n‚úÖ Is the experimental design correctly applied?\n‚úÖ Have treatments been appropriately randomised or assigned comprehensively across subjects?\n‚úÖ What is the population that the collected data describes?\n‚úÖ If the data is observational, can you group them into comparison sets?"
  },
  {
    "objectID": "week3/slides.html#imputing-missing-values",
    "href": "week3/slides.html#imputing-missing-values",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Imputing missing values",
    "text": "Imputing missing values"
  },
  {
    "objectID": "week3/slides.html#example-1-olympic-medals",
    "href": "week3/slides.html#example-1-olympic-medals",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Olympic medals",
    "text": "Example 1: Olympic medals\n\n\n\n\n             country totalmedal\n1       UnitedStates        104\n2              China         88\n3             Russia         82\n4       GreatBritain         65\n5            Germany         44\n6              Japan         38\n7          Australia         35\n8             France         34\n9         SouthKorea         28\n10             Italy         28\n11       Netherlands         20\n12           Ukraine         20\n13            Canada         18\n14           Hungary         17\n15             Spain         17\n16            Brazil         17\n17              Cuba         14\n18        Kazakhstan         13\n19        NewZealand         13\n20              Iran         12\n21           Jamaica         12\n22           Belarus         12\n23             Kenya         11\n24     CzechRepublic         10\n25            Poland         10\n26        Azerbaijan         10\n27           Romania          9\n28           Denmark          9\n29            Sweden          8\n30          Colombia          8\n31          Ethiopia          7\n32            Mexico          7\n33           Georgia          7\n34        NorthKorea          6\n35       SouthAfrica          6\n36           Croatia          6\n37             India          6\n38            Turkey          5\n39         Lithuania          5\n40           Ireland          5\n41          Mongolia          5\n42       Switzerland          4\n43            Norway          4\n44          Slovenia          4\n45            Serbia          4\n46         Argentina          4\n47        Uzbekistan          4\n48 TrinidadandTobago          4\n49          Slovakia          4\n50           Tunisia          3\n51          Thailand          3\n52           Finland          3\n53           Belgium          3\n54           Armenia          3\n55 DominicanRepublic          2\n56            Latvia          2\n57             Egypt          2\n58        PuertoRico          2\n59          Malaysia          2\n60         Indonesia          2\n61           Estonia          2\n62            Taiwan          2\n63          Bulgaria          2\n64         Singapore          2\n65             Qatar          2\n66           Moldova          2\n67            Greece          2\n68         Venezuela          1\n69            Uganda          1\n70           Grenada          1\n71           Bahamas          1\n72           Algeria          1\n73          Portugal          1\n74        Montenegro          1\n75         Guatemala          1\n76             Gabon          1\n77            Cyprus          1\n78          Botswana          1\n79        Tajikistan          1\n80       SaudiArabia          1\n81           Morocco          1\n82            Kuwait          1\n83          HongKong          1\n84           Bahrain          1\n85       Afghanistan          1\n\n\n Is the average number of medals equal to 962/85 = 11.32?\n\n\nWhat is missing?\n\n\nWhat is the correct average number of medals?\n\n\n962/204 = 4.72\n Working out what is missing can be hard!"
  },
  {
    "objectID": "week3/slides.html#example-2-el-nino",
    "href": "week3/slides.html#example-2-el-nino",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: El Nino",
    "text": "Example 2: El Nino\n\n\nExplore missings\n\nplotting on edge of plots, or\nusing simple imputation like mean\n\n\noceanbuoys |&gt;\n  ggplot(aes(x=air_temp_c, y=humidity)) +\n  geom_miss_point()\n\n\n\n\n\n\n\n\n\nImpute and check\n\nImpute using regression or simulation\nCheck distribution relative to complete cases\n\n\n\n\nCode\nlibrary(simputation)\nocean_imp_yr &lt;- oceanbuoys %&gt;%\n  bind_shadow() %&gt;%\n  impute_lm(air_temp_c ~ wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(humidity ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  impute_lm(sea_temp_c ~  wind_ew + wind_ns + year + longitude + latitude) %&gt;%\n  add_label_shadow()\n\nggplot(ocean_imp_yr,\n       aes(x = air_temp_c,\n           y = humidity,\n           color = any_missing)) + \n  geom_point() +\n  theme(legend.title = element_blank())"
  },
  {
    "objectID": "week3/slides.html#validators",
    "href": "week3/slides.html#validators",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Validators",
    "text": "Validators\nAutomating some checks"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (1/3)",
    "text": "Case study: Dutch supermarket revenue and cost (1/3)\n\nData contains the revenue and cost (in Euros) for 60 supermarkets\nData has been anonymised and distorted\n\n\ndata(\"SBS2000\", package = \"validate\")\ndplyr::glimpse(SBS2000)\n\nRows: 60\nColumns: 11\n$ id          &lt;fct&gt; RET01, RET02, RET03, RET04, ‚Ä¶\n$ size        &lt;fct&gt; sc0, sc3, sc3, sc3, sc3, sc0‚Ä¶\n$ incl.prob   &lt;dbl&gt; 0.02, 0.14, 0.14, 0.14, 0.14‚Ä¶\n$ staff       &lt;int&gt; 75, 9, NA, NA, NA, 1, 5, 3, ‚Ä¶\n$ turnover    &lt;int&gt; NA, 1607, 6886, 3861, NA, 25‚Ä¶\n$ other.rev   &lt;int&gt; NA, NA, -33, 13, 37, NA, NA,‚Ä¶\n$ total.rev   &lt;int&gt; 1130, 1607, 6919, 3874, 5602‚Ä¶\n$ staff.costs &lt;int&gt; NA, 131, 324, 290, 314, NA, ‚Ä¶\n$ total.costs &lt;int&gt; 18915, 1544, 6493, 3600, 553‚Ä¶\n$ profit      &lt;int&gt; 20045, 63, 426, 274, 72, 3, ‚Ä¶\n$ vat         &lt;int&gt; NA, NA, NA, NA, NA, NA, 1346‚Ä¶"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (2/3)",
    "text": "Case study: Dutch supermarket revenue and cost (2/3)\n\nChecking for completeness of records\n\n\nlibrary(validate)\nrules &lt;- validator(\n          is_complete(id),\n          is_complete(id, turnover),\n          is_complete(id, turnover, profit))\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     60     0   0 FALSE   FALSE\n2   V2    60     56     4   0 FALSE   FALSE\n3   V3    60     52     8   0 FALSE   FALSE\n                         expression\n1                   is_complete(id)\n2         is_complete(id, turnover)\n3 is_complete(id, turnover, profit)"
  },
  {
    "objectID": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "href": "week3/slides.html#case-study-dutch-supermarket-revenue-and-cost-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Case study: Dutch supermarket revenue and cost (3/3)",
    "text": "Case study: Dutch supermarket revenue and cost (3/3)\n\nSanity check derived variables\n\n\nlibrary(validate)\nrules &lt;- validator(\n    total.rev - profit == total.costs,\n    turnover + other.rev == total.rev,\n    profit &lt;= 0.6 * total.rev\n)\nout &lt;- confront(SBS2000, rules)\nsummary(out)\n\n  name items passes fails nNA error warning\n1   V1    60     39    14   7 FALSE   FALSE\n2   V2    60     19     4  37 FALSE   FALSE\n3   V3    60     49     6   5 FALSE   FALSE\n                                      expression\n1 abs(total.rev - profit - total.costs) &lt;= 1e-08\n2 abs(turnover + other.rev - total.rev) &lt;= 1e-08\n3              profit - 0.6 * total.rev &lt;= 1e-08"
  },
  {
    "objectID": "week3/slides.html#ida-for-hypothesis-testing",
    "href": "week3/slides.html#ida-for-hypothesis-testing",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for hypothesis testing",
    "text": "IDA for hypothesis testing"
  },
  {
    "objectID": "week3/slides.html#hypothesis-testing-13",
    "href": "week3/slides.html#hypothesis-testing-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Hypothesis testing (1/3)",
    "text": "Hypothesis testing (1/3)\n\nState the hypothesis (pair), e.g.¬†\\(H_o: \\mu_1 = \\mu_2\\) vs \\(H_a: \\mu_1 &lt; \\mu_2\\).\nTest statistic depends on assumption about the distribution, e.g.¬†\n\n\\(t\\)-test will assume that distributions are normal, or small departures from if we have a large sample.\ntwo-sample might assume both groups have the same variance\n\n\n\n\nSteps to complete:\n\nCompute the test statistic\nMeasure it against a standard distribution\nIf it is extreme, \\(p\\)-value is small, decision is to reject \\(H_o\\)\n\\(p\\)-value is the probability of observing a value as large as this, or large, assuming \\(H_o\\) is true."
  },
  {
    "objectID": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "href": "week3/slides.html#example-1-checking-variance-and-distribution-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 1: Checking variance and distribution (2/3)",
    "text": "Example 1: Checking variance and distribution (2/3)\n\n\n\n\n\nCode\ndata(sleep)\nggplot(sleep, aes(x=group, y=extra)) + \n  geom_boxplot() +\n  geom_point(colour=\"#D55E00\")\n\n\n\n\n\n\n\n\n\n\nCushny, A. R. and Peebles, A. R. (1905) The action of optical isomers: II hyoscines. The Journal of Physiology 32, 501‚Äì510.\n\n\nFew observations. Nothing strongly suggests violation of normality and spread of points is similar for each group.\n\ntt &lt;- with(sleep,\n     t.test(extra[group == 1],\n            extra[group == 2], \n            paired = TRUE))\ntt\n\n\n    Paired t-test\n\ndata:  extra[group == 1] and extra[group == 2]\nt = -4, df = 9, p-value = 0.003\nalternative hypothesis: true mean difference is not equal to 0\n95 percent confidence interval:\n -2.5 -0.7\nsample estimates:\nmean difference \n           -1.6"
  },
  {
    "objectID": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "href": "week3/slides.html#example-2-checking-distribution-and-variance-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Example 2: Checking distribution and variance (3/3)",
    "text": "Example 2: Checking distribution and variance (3/3)\n\n\n\n\n\nCode\nInsectSprays  |&gt; \n  ggplot(aes(x=fct_reorder(spray, count), \n             y=count)) + \n  geom_jitter(width=0.1, height=0, colour=\"#D55E00\", size=3, alpha=0.8) +\n  xlab(\"\") \n\n\n\n\n\n\n\n\n\n\nIs it plausible that the samples are from a normal population? Do they have equal variance?\n\n\n\nfm1 &lt;- aov(count ~ spray, data = InsectSprays)\nsummary(fm1)\n\n            Df Sum Sq Mean Sq F value Pr(&gt;F)    \nspray        5   2669     534    34.7 &lt;2e-16 ***\nResiduals   66   1015      15                   \n---\nSignif. codes:  \n0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nWhat hypothesis being tested? What would the decision be?\n\n\nWhy does equal variance matter in this test?"
  },
  {
    "objectID": "week3/slides.html#ida-for-inferential-modeling",
    "href": "week3/slides.html#ida-for-inferential-modeling",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "IDA for inferential modeling",
    "text": "IDA for inferential modeling"
  },
  {
    "objectID": "week3/slides.html#linear-models-13",
    "href": "week3/slides.html#linear-models-13",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (1/3)",
    "text": "Linear models (1/3)\n\n\n\n\n\nCode\nlibrary(tidyverse)\nlibrary(broom)\nggplot(cars, aes(speed, dist)) + \n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE)\n\n\n\n\n\n\n\n\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + e_i\\]\nAssumptions:\n\nForm is linear\nError is normally distributed around 0\n\n\n\nCheck using residual plots\n\n\n\nCode\ncars_model &lt;- lm(dist ~ speed, data = cars)\ncars_fit &lt;- augment(cars_model)\n\ncars_p1 &lt;- ggplot(cars_fit, aes(x=.fitted, \n                                y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ncars_p2 &lt;- ggplot(cars_fit, aes(x=.resid)) +\n  geom_density()\ncars_p1 + cars_p2 + plot_layout(ncol=2)"
  },
  {
    "objectID": "week3/slides.html#linear-models-23",
    "href": "week3/slides.html#linear-models-23",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (2/3)",
    "text": "Linear models (2/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 1: fit polynomial form\n\n\\[y_i = \\beta_0 + \\beta_1 x_i + \\beta_2 x_i^2 + e_i.\\]\n\n\nCode\ndiamonds_sub &lt;- diamonds |&gt;\n  filter(carat &lt; 3)\n\ndiamonds_model &lt;- lm(price ~ poly(carat, 2),\n                     data=diamonds_sub)\ndiamonds_fit &lt;- diamonds_sub |&gt;\n  mutate(.fitted = diamonds_model$fitted.values, \n         .resid = diamonds_model$residuals)\n\ndiamonds_p1 &lt;- ggplot(diamonds_fit) +\n  geom_point(aes(x=carat, y=price)) +\n  geom_point(aes(x=carat, y=.fitted),\n             colour=\"#D55E00\")\ndiamonds_p2 &lt;- ggplot(diamonds_fit, \n                      aes(x=.fitted, \n                          y=.resid)) + \n  geom_hline(yintercept = 0, colour=\"grey70\") +\n  geom_point() \ndiamonds_p1 + diamonds_p2 + plot_layout(ncol=2)\n\n\n\n\n\n\n\n\n\n\nForm is not quadratic, continue to explore additional polynomial terms."
  },
  {
    "objectID": "week3/slides.html#linear-models-33",
    "href": "week3/slides.html#linear-models-33",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Linear models (3/3)",
    "text": "Linear models (3/3)\n\n\n\nData and loess smoother\n\n\nCode\nggplot(diamonds, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(se=F)\n\n\n\n\n\n\n\n\n\n\nForm is not linear!\nAlso, insufficient data on large diamonds.\n\n\n\nFix 2: linearise\n\nThe log transformation of both variables linearises the relationship, so that a simple linear model can be used, and can correct heteroskedasticity.\n\n\nCode\nggplot(diamonds_sub, aes(carat, price)) + \n  geom_point(alpha = 0.2) + \n  geom_smooth(method = lm) +\n  scale_x_log10() +\n  scale_y_log10()"
  },
  {
    "objectID": "week3/slides.html#cautions",
    "href": "week3/slides.html#cautions",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Cautions",
    "text": "Cautions\n\nNotice that there was no formal statistical inference when trying to determine an appropriate model form.\n\n\nDiscarded models are hardly ever reported. Consequently, majority of reported statistics give a distorted view and it‚Äôs important to remind yourself what might not be reported."
  },
  {
    "objectID": "week3/slides.html#summary",
    "href": "week3/slides.html#summary",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Summary",
    "text": "Summary\n\n\n\nIDA is a model-focused exploration to support a CDA with:\n\ndata description and collection\ndata quality checking, and\nchecking assumptions\nmodel fit without any formal statistical inference.\n\nIDA is part of EDA, even when no CDA is planned.\n\nIDA may never see the limelight BUT it forms the foundation that the main analysis is built upon. Document it! Do it well!\n\n\n\n\nThe Census Bureau tabulates same-sex couples in both the American Community Survey (ACS) and the Decennial Census. Two questions are used to identify same-sex couples: relationship and sex. The agency follows edit rules that are used to change data values for seemingly contradictory answers. The edit rules for combining information from relationship and sex have evolved since the category of unmarried partner was added in 1990. In that census, if a household consisted of a married couple and both spouses reported the same sex, the relationship category remained husband or wife, but the sex of the partner who reported being a spouse to the householder was changed. Humans all the way down\n\n\n\n\nHuman actions are ubiquitous in every part of data analysis! The most objective methods often have had subjective actions before and after."
  },
  {
    "objectID": "week3/slides.html#further-reading",
    "href": "week3/slides.html#further-reading",
    "title": "ETC5521: Diving Deeply into Data Exploration",
    "section": "Further reading",
    "text": "Further reading\n\nHuebner et al (2018) A Contemporary Conceptual Framework for Initial Data Analysis\nHuebner et al (2020) Hidden analyses\nChatfield (1985) The Initial Examination of Data. Journal of the Royal Statistical Society. Series A (General) 148 \nCox & Snell (1981) Applied Statistics. London: Chapman and Hall.\nvan der Loo and de Jonge (2018). Statistical Data Cleaning with Applications in R. John Wiley and Sons Ltd.\nHyndman (2014) Explaining the ABS unemployment fluctuations\n\n\n\n\nETC5521 Lecture 3 | ddde.numbat.space"
  },
  {
    "objectID": "week4/index.html",
    "href": "week4/index.html",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al.¬†(2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#main-reference",
    "href": "week4/index.html#main-reference",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "",
    "text": "Wickham et al.¬†(2010) Graphical inference for Infovis"
  },
  {
    "objectID": "week4/index.html#what-you-will-learn-this-week",
    "href": "week4/index.html#what-you-will-learn-this-week",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week4/index.html#lecture-slides",
    "href": "week4/index.html#lecture-slides",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week4/index.html#tutorial-instructions",
    "href": "week4/index.html#tutorial-instructions",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week4/index.html#assignments",
    "href": "week4/index.html#assignments",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week4/index.html#assignments-1",
    "href": "week4/index.html#assignments-1",
    "title": "Week 4: Using computational tools to determine whether what is seen in the data can be assumed to apply more broadly",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Monday 26 August."
  },
  {
    "objectID": "week6/index.html",
    "href": "week6/index.html",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#main-reference",
    "href": "week6/index.html#main-reference",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "",
    "text": "Wilke (2019) Ch 12 Visualising associations"
  },
  {
    "objectID": "week6/index.html#what-you-will-learn-this-week",
    "href": "week6/index.html#what-you-will-learn-this-week",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week6/index.html#lecture-slides",
    "href": "week6/index.html#lecture-slides",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week6/index.html#tutorial-instructions",
    "href": "week6/index.html#tutorial-instructions",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week6/index.html#assignments",
    "href": "week6/index.html#assignments",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week6/index.html#assignments-1",
    "href": "week6/index.html#assignments-1",
    "title": "Week 6: Bivariate dependencies and relationships, transformations to linearise",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 2 is due on Monday 26 August."
  },
  {
    "objectID": "week8/index.html",
    "href": "week8/index.html",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#main-reference",
    "href": "week8/index.html#main-reference",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "",
    "text": "Cook and Laa (2023) Interactively exploring high-dimensional data and models in R Chapter 1"
  },
  {
    "objectID": "week8/index.html#what-you-will-learn-this-week",
    "href": "week8/index.html#what-you-will-learn-this-week",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "What you will learn this week",
    "text": "What you will learn this week"
  },
  {
    "objectID": "week8/index.html#lecture-slides",
    "href": "week8/index.html#lecture-slides",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Lecture slides",
    "text": "Lecture slides"
  },
  {
    "objectID": "week8/index.html#tutorial-instructions",
    "href": "week8/index.html#tutorial-instructions",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Tutorial instructions",
    "text": "Tutorial instructions\nInstructions:"
  },
  {
    "objectID": "week8/index.html#assignments",
    "href": "week8/index.html#assignments",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments"
  },
  {
    "objectID": "week8/index.html#assignments-1",
    "href": "week8/index.html#assignments-1",
    "title": "Week 8: Going beyond two variables, exploring high dimensions",
    "section": "Assignments",
    "text": "Assignments\n\nAssignment 3 is due on Monday 16 September."
  },
  {
    "objectID": "week4/tutorial.html",
    "href": "week4/tutorial.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#objectives",
    "href": "week4/tutorial.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorial.html#preparation",
    "href": "week4/tutorial.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorial.html#exercises",
    "href": "week4/tutorial.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\n\n\nlibrary(palmerpenguins)\n\n\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_length_mm. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week‚Äôs lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We‚Äôll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass_g~flipper_length_mm, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?"
  },
  {
    "objectID": "week4/tutorial.html#finishing-up",
    "href": "week4/tutorial.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  },
  {
    "objectID": "week4/tutorialsol.html",
    "href": "week4/tutorialsol.html",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#objectives",
    "href": "week4/tutorialsol.html#objectives",
    "title": "ETC5521 Tutorial 4",
    "section": "",
    "text": "Practice conducting initial data analyses, and make a start on learning how to assess significance of patterns."
  },
  {
    "objectID": "week4/tutorialsol.html#preparation",
    "href": "week4/tutorialsol.html#preparation",
    "title": "ETC5521 Tutorial 4",
    "section": "üîß Preparation",
    "text": "üîß Preparation\nThe reading for this week is The initial examination of data. It is authored by Chris Chatfield, and is a classic paper explaining the role of initial data analysis. - Complete the weekly quiz, before the deadline! - Make sure you have this list of R packages installed:\n\nComplete the weekly quiz, before the deadline!\nMake sure you have this list of R packages installed:\n\n\ninstall.packages(c(\"tidyverse\", \"palmerpenguins\", \"ggbeeswarm\", \"broom\", \"nullabor\"))\n\n\nNote that the code and data for reproducing their analysis can be found here.\nOpen your RStudio Project for this unit, (the one you created in week 1, ETC5521). Create a .qmd document for this weeks activities."
  },
  {
    "objectID": "week4/tutorialsol.html#exercises",
    "href": "week4/tutorialsol.html#exercises",
    "title": "ETC5521 Tutorial 4",
    "section": "üì• Exercises",
    "text": "üì• Exercises\n\nExercise 1: IDA on penguins data\n\nTake a glimpse of the penguins data. What types are variables are present in the data?\n\n\nlibrary(palmerpenguins)\n\n\nHow was this data collected? You will need to read the documentation for the palmerpenguins package.\nUsing the visdat package make an overview plot to examine types of variables and for missing values.\nCheck the distributions of each species on each of the size variables, using a jittered dotplot, using the geom_quasirandom() function in the ggbeeswarm package. There seems to be some bimodality in some species on some variables eg bill_length_mm. Why do you think this might be? Check your thinking by making a suitable plot.\nIs there any indication of outliers from the jittered dotplots of different variables?\nMake a scatterplot of body_mass_g vs flipper_length_mm for all the penguins. What do the vertical stripes indicate? Are there any other unusual patterns to note, such as outliers or clustering or nonlinearity?\nHow well can penguin body mass be predicted based on the flipper length? Fit a linear model to check. Report the equation, the \\(R^2\\), \\(\\sigma\\), and make a residual plot of residuals vs flipper_length_mm. From the residual plot, are there any concerns about the model fit?\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nglimpse(penguins)\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel‚Ä¶\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse‚Ä¶\n$ bill_length_mm    &lt;dbl&gt; 39, 40, 40, NA, 37, 39, 39, 39, 34, 42, 38, 38, 41, ‚Ä¶\n$ bill_depth_mm     &lt;dbl&gt; 19, 17, 18, NA, 19, 21, 18, 20, 18, 20, 17, 17, 18, ‚Ä¶\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186‚Ä¶\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, ‚Ä¶\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male‚Ä¶\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007‚Ä¶\n\n\n\nDetails are at https://allisonhorst.github.io/palmerpenguins/articles/intro.html, and you learn ‚ÄúThese data were collected from 2007 - 2009 by Dr.¬†Kristen Gorman with the Palmer Station Long Term Ecological Research Program, part of the US Long Term Ecological Research Network. The data were imported directly from the Environmental Data Initiative (EDI) Data Portal.‚Äù It is necessary to also read the original data collection article https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0090081 to obtain details of the sampling. Breeding pairs of penguins were included based on sampling of nests where pairs of adults were present, were chosen and marked, before onset of egg-laying.\nThere are three factor variables - species, island, sex - and three integer variables - flipper_length_mm, body_mass_g and year - and two numeric variables - bill_length_mm and bill_depth_mm.\n\nIt is interesting that flipper_length_mm and body_mass_g are reported without decimal places, and thus are integers, whereas bill_length_mm and bill_depth_mm are reported with one decimal place, and thus are doubles. Both are numeric variables, though.\nFour variables have missing values: sex, bill_length_mm, bill_depth_mm, flipper_length_mm, body_mass_g. There are more missings on sex. The other missings occur together, the penguins are missing on all five variables.\n\nlibrary(visdat)\nvis_dat(penguins)\n\n\n\n\n\n\n\n\n\nParticularly on bill_length_mm multimodality can be seen in the Chinstrap and Gentoo penguins. This corresponds to differences in the two species. Differences can be seen in the sexes for all of the variables and species, but it is only big enough in bill_length_mm to be noticeable as bimodality.\n\n\nlibrary(ggbeeswarm)\nggplot(penguins, aes(x=species, \n                     y=bill_length_mm)) +\n  geom_quasirandom() \n\n\n\n\n\n\n\nggplot(penguins, aes(x=species, \n                     y=bill_length_mm, \n                     colour=sex)) +\n  geom_quasirandom() +\n  scale_color_brewer(\"\", palette=\"Dark2\")\n\n\n\n\n\n\n\n\n\nOutliers can be seen on bill_length_mm for Chinstrap and Gentoo. Interestingly, there is one female penguins with a really big bill, bigger than all the males even.\n\nAlso on flipper_length_mm there is one female penguins with a much smaller value than others.\n\nThe striping corresponds to rounded values of flipper length, that have been reported to the nearest mm. It appears that it is done across all measuring as it is present for both sexes, for all species, for each island and each year. Otherwise there is nothing much to report as unusual.\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~island, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~sex, ncol=2, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~species, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\npenguins %&gt;%\n  na.omit() %&gt;%\n  ggplot(aes(x=flipper_length_mm,\n             y=body_mass_g)) +\n  geom_point() +\n  facet_wrap(~year, ncol=3, scales = \"free\") +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\nThe model fit statistics suggest it is a reasonably good model, with flipper length explaining about 76% of body mass. From the estimated standard deviation of the error, \\(\\sigma=393\\), we could say that the estimated body mass is likely accurate to within 800g. (Assuming normal distribution and 95% of observations within two \\(\\sigma\\).)\n\nThe residual suggests no major problems. There is a little heteroskedasticity. Perhaps if you look carefully, though, it might indicate that different models should have been fitted for the smaller penguins and the bigger penguins, perhaps models separately for sex and species would be advised.\n\nlibrary(broom)\npenguins_nona &lt;- penguins %&gt;%\n  na.omit()\npenguins_fit &lt;- lm(body_mass_g~flipper_length_mm, data=penguins_nona)\ntidy(penguins_fit)\n\n# A tibble: 2 √ó 5\n  term              estimate std.error statistic   p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -5872.     310.       -18.9 1.18e- 54\n2 flipper_length_mm     50.2      1.54      32.6 3.13e-105\n\nglance(penguins_fit)\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic   p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.762         0.761  393.     1060. 3.13e-105     1 -2461. 4928. 4940.\n# ‚Ñπ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\npenguins_m &lt;- augment(penguins_fit)\nggplot(penguins_m, aes(x=flipper_length_mm, y=.resid)) +\n  geom_hline(yintercept=1, colour=\"grey70\") +\n  geom_point() +\n  theme(aspect.ratio=1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExercise 2: Can we believe what we see?\nThis question uses material from this week‚Äôs lecture, from a few hours ago.\n\nIn the previous question we made subjective statements about the residual plot to determine if the model was a good fit or not. We‚Äôll use randomisation to check any observations we made from the residual plot. The code below makes a lineup of the true plot against plots made with rotation residuals (nulls/good). When you run the code you will get a line decrypt(\"....\"), which you can copy and paste back in to the console window to get the location of the true plot (in case you forgot which it is). Does the true plot look like the null plots? If not, describe how it differs.\n\n\nlibrary(nullabor)\nggplot(lineup(null_lm(body_mass_g~flipper_length_mm, method=\"rotate\"),\n              penguins_m),\n       aes(x=flipper_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\nPick one group, males or females, and one of Adelie, Chinstrap or Gentoo, and choose two of the four measurements. Fit a linear model, and do a lineup of the residuals. Can you tell which is the true plot? Show your lineup to your tutorial partner or someone else nearby and ask them\n\n\nto pick the plot that is most different.\nexplain why they picked that plot.\n\nUsing your decrypt() code locate the true plot. Is the true plot different from the nulls?\nDid you or your friend choose the data plot? Was it identifiable from the lineup or indistinguishable from the null plots?\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\n\nThe true plot looks a little different from the nulls. It has more of a V shape, which might suggest that the model fits poorly, that the smaller penguins have a different relationship than the larger penguins. It is evidence to support fitting separate models to the sexes and species.\nSomething like the following\n\n\nlibrary(nullabor)\npenguins_f_adelie &lt;- penguins_nona %&gt;%\n  filter(species == \"Adelie\",\n         sex == \"female\")\npenguins_f_adelie_bl_bd_fit &lt;- \n  lm(bill_depth_mm ~ bill_length_mm,\n     data=penguins_f_adelie)\npenguins_f_adelie_bl_bd_m &lt;-\n  augment(penguins_f_adelie_bl_bd_fit)\nggplot(lineup(null_lm(bill_depth_mm ~ \n                        bill_length_mm,\n                      method=\"rotate\"),\n              penguins_f_adelie_bl_bd_m),\n       aes(x=bill_length_mm, y=.resid)) +\n  geom_point() +\n  facet_wrap(~.sample, ncol=5) +\n  theme_void() +\n  theme(axis.text = element_blank(), \n        panel.border = element_rect(fill=NA, colour=\"black\"))\n\n\n\n\n\n\n\n\nI would not be able to distinguish which is the true plot in this lineup."
  },
  {
    "objectID": "week4/tutorialsol.html#finishing-up",
    "href": "week4/tutorialsol.html#finishing-up",
    "title": "ETC5521 Tutorial 4",
    "section": "üëå Finishing up",
    "text": "üëå Finishing up\nMake sure you say thanks and good-bye to your tutor. This is a time to also report what you enjoyed and what you found difficult."
  }
]